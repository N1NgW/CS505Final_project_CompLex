{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS505FinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK0gMeXPKEEX"
      },
      "source": [
        "import re\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from copy import deepcopy\n",
        "import math\n",
        "\n",
        "import torchtext.vocab\n",
        "\n",
        "import string\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import sklearn\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYtWoHTOKcNk",
        "outputId": "d30a210f-064c-43cd-92a7-663ea6512c05"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNkKxGDiKcIN"
      },
      "source": [
        "# the paths; change when necessary\n",
        "TRAIN_RAW = \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/lcp_single_train.tsv\"\n",
        "TEST_RAW = \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/lcp_single_test.tsv\"\n",
        "TRAIN = \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/lcp_single_train_cleaned.tsv\"\n",
        "TEST = \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/lcp_single_test_cleaned.tsv\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVQ9mky8KcFb"
      },
      "source": [
        "# read the datasets\n",
        "# train\n",
        "with open(TRAIN_RAW, 'r') as f:\n",
        "  data = f.read()\n",
        "\n",
        "# need to remove \" from the string, otherwise parsing will have problems because some quotas are not closed \n",
        "data = data.replace('\"', '')\n",
        "\n",
        "with open(TRAIN, 'w') as f:\n",
        "  f.write(data)\n",
        "\n",
        "df = pd.read_csv(TRAIN, sep='\\t')\n",
        "\n",
        "# test\n",
        "with open(TEST_RAW, 'r') as f:\n",
        "  data = f.read()\n",
        "\n",
        "data = data.replace('\"', '')\n",
        "\n",
        "with open(TEST, 'w') as f:\n",
        "  f.write(data)\n",
        "\n",
        "test = pd.read_csv(TEST, sep='\\t')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Vw8Gk02zKcCc",
        "outputId": "dbb9687c-de0e-4bd8-bba3-b701615cab6d"
      },
      "source": [
        "# take a look\n",
        "pd.set_option('display.max_colwidth', None) # show the whole sentence\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>corpus</th>\n",
              "      <th>sentence</th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3ZLW647WALVGE8EBR50EGUBPU4P32A</td>\n",
              "      <td>bible</td>\n",
              "      <td>Behold, there came up out of the river seven cattle, sleek and fat, and they fed in the marsh grass.</td>\n",
              "      <td>river</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>34R0BODSP1ZBN3DVY8J8XSIY551E5C</td>\n",
              "      <td>bible</td>\n",
              "      <td>I am a fellow bondservant with you and with your brothers, the prophets, and with those who keep the words of this book.</td>\n",
              "      <td>brothers</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3S1WOPCJFGTJU2SGNAN2Y213N6WJE3</td>\n",
              "      <td>bible</td>\n",
              "      <td>The man, the lord of the land, said to us, 'By this I will know that you are honest men: leave one of your brothers with me, and take grain for the famine of your houses, and go your way.</td>\n",
              "      <td>brothers</td>\n",
              "      <td>0.050000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3BFNCI9LYKQN09BHXHH9CLSX5KP738</td>\n",
              "      <td>bible</td>\n",
              "      <td>Shimei had sixteen sons and six daughters; but his brothers didn't have many children, neither did all their family multiply like the children of Judah.</td>\n",
              "      <td>brothers</td>\n",
              "      <td>0.150000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3G5RUKN2EC3YIWSKUXZ8ZVH95R49N2</td>\n",
              "      <td>bible</td>\n",
              "      <td>He has put my brothers far from me.</td>\n",
              "      <td>brothers</td>\n",
              "      <td>0.263889</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               id corpus  ...     token complexity\n",
              "0  3ZLW647WALVGE8EBR50EGUBPU4P32A  bible  ...     river   0.000000\n",
              "1  34R0BODSP1ZBN3DVY8J8XSIY551E5C  bible  ...  brothers   0.000000\n",
              "2  3S1WOPCJFGTJU2SGNAN2Y213N6WJE3  bible  ...  brothers   0.050000\n",
              "3  3BFNCI9LYKQN09BHXHH9CLSX5KP738  bible  ...  brothers   0.150000\n",
              "4  3G5RUKN2EC3YIWSKUXZ8ZVH95R49N2  bible  ...  brothers   0.263889\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "UTLwl2AWa-XG",
        "outputId": "90993481-fac4-4dcd-8783-e70e5e3fc2d2"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>corpus</th>\n",
              "      <th>sentence</th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3K8CQCU3KE19US5SN890DFPK3SANWR</td>\n",
              "      <td>bible</td>\n",
              "      <td>But he, beckoning to them with his hand to be silent, declared to them how the Lord had brought him out of the prison.</td>\n",
              "      <td>hand</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3Q2T3FD0ON86LCI41NJYV3PN0BW3MV</td>\n",
              "      <td>bible</td>\n",
              "      <td>If I forget you, Jerusalem, let my right hand forget its skill.</td>\n",
              "      <td>hand</td>\n",
              "      <td>0.197368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3ULIZ0H1VA5C32JJMKOTQ8Z4GUS51B</td>\n",
              "      <td>bible</td>\n",
              "      <td>the ten sons of Haman the son of Hammedatha, the Jew's enemy, but they didn't lay their hand on the plunder.</td>\n",
              "      <td>hand</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3BFF0DJK8XCEIOT30ZLBPPSRMZQTSD</td>\n",
              "      <td>bible</td>\n",
              "      <td>Let your hand be lifted up above your adversaries, and let all of your enemies be cut off.</td>\n",
              "      <td>hand</td>\n",
              "      <td>0.267857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3QREJ3J433XSBS8QMHAICCR0BQ1LKR</td>\n",
              "      <td>bible</td>\n",
              "      <td>Abimelech chased him, and he fled before him, and many fell wounded, even to the entrance of the gate.</td>\n",
              "      <td>entrance</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               id corpus  ...     token complexity\n",
              "0  3K8CQCU3KE19US5SN890DFPK3SANWR  bible  ...      hand   0.000000\n",
              "1  3Q2T3FD0ON86LCI41NJYV3PN0BW3MV  bible  ...      hand   0.197368\n",
              "2  3ULIZ0H1VA5C32JJMKOTQ8Z4GUS51B  bible  ...      hand   0.200000\n",
              "3  3BFF0DJK8XCEIOT30ZLBPPSRMZQTSD  bible  ...      hand   0.267857\n",
              "4  3QREJ3J433XSBS8QMHAICCR0BQ1LKR  bible  ...  entrance   0.000000\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN3ToWiZQlyk"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPdQRWvvaQyX"
      },
      "source": [
        "Try linear regression first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYqivG31Lubb"
      },
      "source": [
        "def create_weights_matrix(vocab, dimension=100):\n",
        "  \"\"\" create a matrix containing vectors for each word in Glove \"\"\"\n",
        "  matrix_len = len(vocab)\n",
        "  weights_matrix = np.zeros((matrix_len, dimension))\n",
        "\n",
        "  for i, word in enumerate(vocab):\n",
        "      try: \n",
        "          weights_matrix[i] = glove[word]\n",
        "      except KeyError:\n",
        "          weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, )) # initialize a random vector\n",
        "  #return torch.from_numpy(weights_matrix) # a tensor\n",
        "  return weights_matrix"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhOsDaEgacAF"
      },
      "source": [
        "# use the Glove 6B 100d\n",
        "cache_dir = \"/content/gdrive/My Drive/Colab Notebooks/data\"\n",
        "# glove = vocab.pretrained_aliases[\"glove.6B.100d\"](cache=cache_dir)\n",
        "glove = torchtext.vocab.GloVe(name='6B', dim=100, cache=cache_dir)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgCVuZWyZEXm",
        "outputId": "384488a2-fed9-401b-863d-d98ed2304ab9"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0x_EbtYiab9v",
        "outputId": "6a988af6-e079-4dc0-bc91-6abcfbc3baed"
      },
      "source": [
        "# get all the non-unique tokens for prediction\n",
        "tokens = df['token'].dropna().to_list()\n",
        "tokens = [token.lower() for token in tokens] # lowercase\n",
        "print(len(tokens))\n",
        "\n",
        "# check if all tokens are in Glove\n",
        "for token in tokens:\n",
        "  if token not in glove.stoi:\n",
        "    print(\"Token Not Found: \")\n",
        "    print(token)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7659\n",
            "Token Not Found: \n",
            "perverseness\n",
            "Token Not Found: \n",
            "perverseness\n",
            "Token Not Found: \n",
            "perverseness\n",
            "Token Not Found: \n",
            "housetops\n",
            "Token Not Found: \n",
            "slanderers\n",
            "Token Not Found: \n",
            "plowmen\n",
            "Token Not Found: \n",
            "dainties\n",
            "Token Not Found: \n",
            "dainties\n",
            "Token Not Found: \n",
            "dainties\n",
            "Token Not Found: \n",
            "dainties\n",
            "Token Not Found: \n",
            "dainties\n",
            "Token Not Found: \n",
            "dunghill\n",
            "Token Not Found: \n",
            "carotids\n",
            "Token Not Found: \n",
            "tace\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "aaZuP5QmeGgf",
        "outputId": "d9056f73-a1bb-4c59-b54e-f80a020286fb"
      },
      "source": [
        "# create a dataframe for linear regression\n",
        "train_df = pd.DataFrame(tokens, columns =['token'])\n",
        "\n",
        "# add back complexity\n",
        "train_df['complexity'] = df['complexity']\n",
        "\n",
        "# word length\n",
        "train_df['word_length'] = train_df['token'].map(lambda x: len(x))\n",
        "\n",
        "# punctuations\n",
        "punc = string.punctuation\n",
        "\n",
        "# stop words\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "# word frequency\n",
        "# tokenize the whole curpus\n",
        "temp = df['sentence'].to_list()\n",
        "texts = []\n",
        "for sent in temp:\n",
        "  sent = sent.lower()\n",
        "  sent = ''.join([c for c in sent if c not in punc])\n",
        "  words = [word for word in sent.split(' ') if (word.isalpha() and word not in stop_words)]\n",
        "  texts += words\n",
        "# count frequency\n",
        "count = Counter(texts)\n",
        "train_df['word_frequency'] = train_df['token'].map(lambda x: count[x])\n",
        "\n",
        "train_df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>word_length</th>\n",
              "      <th>word_frequency</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>river</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.263889</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      token  complexity  word_length  word_frequency\n",
              "0     river    0.000000            5              26\n",
              "1  brothers    0.000000            8              36\n",
              "2  brothers    0.050000            8              36\n",
              "3  brothers    0.150000            8              36\n",
              "4  brothers    0.263889            8              36"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "KU-Kz_ThLuYD",
        "outputId": "1d8a000d-db48-4d61-da43-98e517cdc108"
      },
      "source": [
        "# create the weight matrix\n",
        "weight_matrix = create_weights_matrix(tokens)\n",
        "print(weight_matrix.shape)\n",
        "\n",
        "# combine\n",
        "weight_matrix_df = pd.DataFrame(weight_matrix)\n",
        "\n",
        "train_df_combined = pd.concat([train_df, weight_matrix_df], axis=1)\n",
        "train_df_combined.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7659, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>word_length</th>\n",
              "      <th>word_frequency</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>river</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5</td>\n",
              "      <td>26</td>\n",
              "      <td>-0.33249</td>\n",
              "      <td>-0.56631</td>\n",
              "      <td>0.54255</td>\n",
              "      <td>-0.11869</td>\n",
              "      <td>0.531290</td>\n",
              "      <td>-0.49381</td>\n",
              "      <td>0.64114</td>\n",
              "      <td>0.85982</td>\n",
              "      <td>0.39633</td>\n",
              "      <td>-1.53950</td>\n",
              "      <td>-0.30613</td>\n",
              "      <td>0.97267</td>\n",
              "      <td>-0.31192</td>\n",
              "      <td>-0.10311</td>\n",
              "      <td>0.359510</td>\n",
              "      <td>-0.60023</td>\n",
              "      <td>0.909830</td>\n",
              "      <td>-0.959540</td>\n",
              "      <td>-0.55375</td>\n",
              "      <td>0.082818</td>\n",
              "      <td>0.26711</td>\n",
              "      <td>0.64645</td>\n",
              "      <td>-0.098556</td>\n",
              "      <td>0.539240</td>\n",
              "      <td>-0.21810</td>\n",
              "      <td>-0.13430</td>\n",
              "      <td>-1.80700</td>\n",
              "      <td>-0.14879</td>\n",
              "      <td>0.39006</td>\n",
              "      <td>-0.62883</td>\n",
              "      <td>-0.38825</td>\n",
              "      <td>0.31925</td>\n",
              "      <td>0.77853</td>\n",
              "      <td>-0.60273</td>\n",
              "      <td>0.063585</td>\n",
              "      <td>-0.75916</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.53185</td>\n",
              "      <td>0.72585</td>\n",
              "      <td>0.36811</td>\n",
              "      <td>0.19494</td>\n",
              "      <td>0.64276</td>\n",
              "      <td>0.81460</td>\n",
              "      <td>0.26748</td>\n",
              "      <td>-0.39275</td>\n",
              "      <td>0.425950</td>\n",
              "      <td>0.11699</td>\n",
              "      <td>0.21063</td>\n",
              "      <td>-0.061747</td>\n",
              "      <td>0.79298</td>\n",
              "      <td>-0.45978</td>\n",
              "      <td>0.85176</td>\n",
              "      <td>-0.36726</td>\n",
              "      <td>0.11816</td>\n",
              "      <td>0.504160</td>\n",
              "      <td>-0.065352</td>\n",
              "      <td>0.69672</td>\n",
              "      <td>0.37525</td>\n",
              "      <td>0.92586</td>\n",
              "      <td>-0.83036</td>\n",
              "      <td>-0.087948</td>\n",
              "      <td>-0.49715</td>\n",
              "      <td>0.21411</td>\n",
              "      <td>-0.82838</td>\n",
              "      <td>-0.85912</td>\n",
              "      <td>0.61576</td>\n",
              "      <td>1.18800</td>\n",
              "      <td>-0.30745</td>\n",
              "      <td>-1.20090</td>\n",
              "      <td>-1.70970</td>\n",
              "      <td>0.51400</td>\n",
              "      <td>-1.01590</td>\n",
              "      <td>0.55555</td>\n",
              "      <td>-1.03850</td>\n",
              "      <td>-0.69940</td>\n",
              "      <td>1.050600</td>\n",
              "      <td>0.24051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "      <td>0.85968</td>\n",
              "      <td>-0.39038</td>\n",
              "      <td>-0.62678</td>\n",
              "      <td>-0.55279</td>\n",
              "      <td>0.097012</td>\n",
              "      <td>0.00658</td>\n",
              "      <td>-0.65021</td>\n",
              "      <td>-0.58272</td>\n",
              "      <td>-1.27630</td>\n",
              "      <td>0.11251</td>\n",
              "      <td>0.78504</td>\n",
              "      <td>0.16027</td>\n",
              "      <td>0.38327</td>\n",
              "      <td>0.62672</td>\n",
              "      <td>-0.017462</td>\n",
              "      <td>-0.36443</td>\n",
              "      <td>0.062441</td>\n",
              "      <td>0.039266</td>\n",
              "      <td>-0.47318</td>\n",
              "      <td>0.547680</td>\n",
              "      <td>0.42916</td>\n",
              "      <td>-0.25516</td>\n",
              "      <td>0.100900</td>\n",
              "      <td>0.041618</td>\n",
              "      <td>-0.14579</td>\n",
              "      <td>0.15174</td>\n",
              "      <td>-0.54301</td>\n",
              "      <td>-0.29787</td>\n",
              "      <td>0.36268</td>\n",
              "      <td>0.89550</td>\n",
              "      <td>0.65319</td>\n",
              "      <td>0.40141</td>\n",
              "      <td>0.03668</td>\n",
              "      <td>-0.34313</td>\n",
              "      <td>-0.102040</td>\n",
              "      <td>-0.19383</td>\n",
              "      <td>...</td>\n",
              "      <td>0.65085</td>\n",
              "      <td>0.97366</td>\n",
              "      <td>0.36997</td>\n",
              "      <td>-0.58266</td>\n",
              "      <td>0.58389</td>\n",
              "      <td>-0.62574</td>\n",
              "      <td>-0.24252</td>\n",
              "      <td>1.37500</td>\n",
              "      <td>-0.042651</td>\n",
              "      <td>0.16398</td>\n",
              "      <td>-0.53462</td>\n",
              "      <td>0.552750</td>\n",
              "      <td>-0.58019</td>\n",
              "      <td>-0.78386</td>\n",
              "      <td>-0.18787</td>\n",
              "      <td>-0.20305</td>\n",
              "      <td>0.11506</td>\n",
              "      <td>-0.089296</td>\n",
              "      <td>-0.766080</td>\n",
              "      <td>0.04339</td>\n",
              "      <td>0.50251</td>\n",
              "      <td>0.73799</td>\n",
              "      <td>0.23388</td>\n",
              "      <td>0.200380</td>\n",
              "      <td>-0.93906</td>\n",
              "      <td>-0.33974</td>\n",
              "      <td>-0.56534</td>\n",
              "      <td>-0.95945</td>\n",
              "      <td>-0.14597</td>\n",
              "      <td>-0.35173</td>\n",
              "      <td>-0.40463</td>\n",
              "      <td>-0.32671</td>\n",
              "      <td>0.24982</td>\n",
              "      <td>-0.27804</td>\n",
              "      <td>-0.99877</td>\n",
              "      <td>-0.39367</td>\n",
              "      <td>-0.30087</td>\n",
              "      <td>-0.24623</td>\n",
              "      <td>0.006483</td>\n",
              "      <td>-0.21982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "      <td>0.85968</td>\n",
              "      <td>-0.39038</td>\n",
              "      <td>-0.62678</td>\n",
              "      <td>-0.55279</td>\n",
              "      <td>0.097012</td>\n",
              "      <td>0.00658</td>\n",
              "      <td>-0.65021</td>\n",
              "      <td>-0.58272</td>\n",
              "      <td>-1.27630</td>\n",
              "      <td>0.11251</td>\n",
              "      <td>0.78504</td>\n",
              "      <td>0.16027</td>\n",
              "      <td>0.38327</td>\n",
              "      <td>0.62672</td>\n",
              "      <td>-0.017462</td>\n",
              "      <td>-0.36443</td>\n",
              "      <td>0.062441</td>\n",
              "      <td>0.039266</td>\n",
              "      <td>-0.47318</td>\n",
              "      <td>0.547680</td>\n",
              "      <td>0.42916</td>\n",
              "      <td>-0.25516</td>\n",
              "      <td>0.100900</td>\n",
              "      <td>0.041618</td>\n",
              "      <td>-0.14579</td>\n",
              "      <td>0.15174</td>\n",
              "      <td>-0.54301</td>\n",
              "      <td>-0.29787</td>\n",
              "      <td>0.36268</td>\n",
              "      <td>0.89550</td>\n",
              "      <td>0.65319</td>\n",
              "      <td>0.40141</td>\n",
              "      <td>0.03668</td>\n",
              "      <td>-0.34313</td>\n",
              "      <td>-0.102040</td>\n",
              "      <td>-0.19383</td>\n",
              "      <td>...</td>\n",
              "      <td>0.65085</td>\n",
              "      <td>0.97366</td>\n",
              "      <td>0.36997</td>\n",
              "      <td>-0.58266</td>\n",
              "      <td>0.58389</td>\n",
              "      <td>-0.62574</td>\n",
              "      <td>-0.24252</td>\n",
              "      <td>1.37500</td>\n",
              "      <td>-0.042651</td>\n",
              "      <td>0.16398</td>\n",
              "      <td>-0.53462</td>\n",
              "      <td>0.552750</td>\n",
              "      <td>-0.58019</td>\n",
              "      <td>-0.78386</td>\n",
              "      <td>-0.18787</td>\n",
              "      <td>-0.20305</td>\n",
              "      <td>0.11506</td>\n",
              "      <td>-0.089296</td>\n",
              "      <td>-0.766080</td>\n",
              "      <td>0.04339</td>\n",
              "      <td>0.50251</td>\n",
              "      <td>0.73799</td>\n",
              "      <td>0.23388</td>\n",
              "      <td>0.200380</td>\n",
              "      <td>-0.93906</td>\n",
              "      <td>-0.33974</td>\n",
              "      <td>-0.56534</td>\n",
              "      <td>-0.95945</td>\n",
              "      <td>-0.14597</td>\n",
              "      <td>-0.35173</td>\n",
              "      <td>-0.40463</td>\n",
              "      <td>-0.32671</td>\n",
              "      <td>0.24982</td>\n",
              "      <td>-0.27804</td>\n",
              "      <td>-0.99877</td>\n",
              "      <td>-0.39367</td>\n",
              "      <td>-0.30087</td>\n",
              "      <td>-0.24623</td>\n",
              "      <td>0.006483</td>\n",
              "      <td>-0.21982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "      <td>0.85968</td>\n",
              "      <td>-0.39038</td>\n",
              "      <td>-0.62678</td>\n",
              "      <td>-0.55279</td>\n",
              "      <td>0.097012</td>\n",
              "      <td>0.00658</td>\n",
              "      <td>-0.65021</td>\n",
              "      <td>-0.58272</td>\n",
              "      <td>-1.27630</td>\n",
              "      <td>0.11251</td>\n",
              "      <td>0.78504</td>\n",
              "      <td>0.16027</td>\n",
              "      <td>0.38327</td>\n",
              "      <td>0.62672</td>\n",
              "      <td>-0.017462</td>\n",
              "      <td>-0.36443</td>\n",
              "      <td>0.062441</td>\n",
              "      <td>0.039266</td>\n",
              "      <td>-0.47318</td>\n",
              "      <td>0.547680</td>\n",
              "      <td>0.42916</td>\n",
              "      <td>-0.25516</td>\n",
              "      <td>0.100900</td>\n",
              "      <td>0.041618</td>\n",
              "      <td>-0.14579</td>\n",
              "      <td>0.15174</td>\n",
              "      <td>-0.54301</td>\n",
              "      <td>-0.29787</td>\n",
              "      <td>0.36268</td>\n",
              "      <td>0.89550</td>\n",
              "      <td>0.65319</td>\n",
              "      <td>0.40141</td>\n",
              "      <td>0.03668</td>\n",
              "      <td>-0.34313</td>\n",
              "      <td>-0.102040</td>\n",
              "      <td>-0.19383</td>\n",
              "      <td>...</td>\n",
              "      <td>0.65085</td>\n",
              "      <td>0.97366</td>\n",
              "      <td>0.36997</td>\n",
              "      <td>-0.58266</td>\n",
              "      <td>0.58389</td>\n",
              "      <td>-0.62574</td>\n",
              "      <td>-0.24252</td>\n",
              "      <td>1.37500</td>\n",
              "      <td>-0.042651</td>\n",
              "      <td>0.16398</td>\n",
              "      <td>-0.53462</td>\n",
              "      <td>0.552750</td>\n",
              "      <td>-0.58019</td>\n",
              "      <td>-0.78386</td>\n",
              "      <td>-0.18787</td>\n",
              "      <td>-0.20305</td>\n",
              "      <td>0.11506</td>\n",
              "      <td>-0.089296</td>\n",
              "      <td>-0.766080</td>\n",
              "      <td>0.04339</td>\n",
              "      <td>0.50251</td>\n",
              "      <td>0.73799</td>\n",
              "      <td>0.23388</td>\n",
              "      <td>0.200380</td>\n",
              "      <td>-0.93906</td>\n",
              "      <td>-0.33974</td>\n",
              "      <td>-0.56534</td>\n",
              "      <td>-0.95945</td>\n",
              "      <td>-0.14597</td>\n",
              "      <td>-0.35173</td>\n",
              "      <td>-0.40463</td>\n",
              "      <td>-0.32671</td>\n",
              "      <td>0.24982</td>\n",
              "      <td>-0.27804</td>\n",
              "      <td>-0.99877</td>\n",
              "      <td>-0.39367</td>\n",
              "      <td>-0.30087</td>\n",
              "      <td>-0.24623</td>\n",
              "      <td>0.006483</td>\n",
              "      <td>-0.21982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.263889</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "      <td>0.85968</td>\n",
              "      <td>-0.39038</td>\n",
              "      <td>-0.62678</td>\n",
              "      <td>-0.55279</td>\n",
              "      <td>0.097012</td>\n",
              "      <td>0.00658</td>\n",
              "      <td>-0.65021</td>\n",
              "      <td>-0.58272</td>\n",
              "      <td>-1.27630</td>\n",
              "      <td>0.11251</td>\n",
              "      <td>0.78504</td>\n",
              "      <td>0.16027</td>\n",
              "      <td>0.38327</td>\n",
              "      <td>0.62672</td>\n",
              "      <td>-0.017462</td>\n",
              "      <td>-0.36443</td>\n",
              "      <td>0.062441</td>\n",
              "      <td>0.039266</td>\n",
              "      <td>-0.47318</td>\n",
              "      <td>0.547680</td>\n",
              "      <td>0.42916</td>\n",
              "      <td>-0.25516</td>\n",
              "      <td>0.100900</td>\n",
              "      <td>0.041618</td>\n",
              "      <td>-0.14579</td>\n",
              "      <td>0.15174</td>\n",
              "      <td>-0.54301</td>\n",
              "      <td>-0.29787</td>\n",
              "      <td>0.36268</td>\n",
              "      <td>0.89550</td>\n",
              "      <td>0.65319</td>\n",
              "      <td>0.40141</td>\n",
              "      <td>0.03668</td>\n",
              "      <td>-0.34313</td>\n",
              "      <td>-0.102040</td>\n",
              "      <td>-0.19383</td>\n",
              "      <td>...</td>\n",
              "      <td>0.65085</td>\n",
              "      <td>0.97366</td>\n",
              "      <td>0.36997</td>\n",
              "      <td>-0.58266</td>\n",
              "      <td>0.58389</td>\n",
              "      <td>-0.62574</td>\n",
              "      <td>-0.24252</td>\n",
              "      <td>1.37500</td>\n",
              "      <td>-0.042651</td>\n",
              "      <td>0.16398</td>\n",
              "      <td>-0.53462</td>\n",
              "      <td>0.552750</td>\n",
              "      <td>-0.58019</td>\n",
              "      <td>-0.78386</td>\n",
              "      <td>-0.18787</td>\n",
              "      <td>-0.20305</td>\n",
              "      <td>0.11506</td>\n",
              "      <td>-0.089296</td>\n",
              "      <td>-0.766080</td>\n",
              "      <td>0.04339</td>\n",
              "      <td>0.50251</td>\n",
              "      <td>0.73799</td>\n",
              "      <td>0.23388</td>\n",
              "      <td>0.200380</td>\n",
              "      <td>-0.93906</td>\n",
              "      <td>-0.33974</td>\n",
              "      <td>-0.56534</td>\n",
              "      <td>-0.95945</td>\n",
              "      <td>-0.14597</td>\n",
              "      <td>-0.35173</td>\n",
              "      <td>-0.40463</td>\n",
              "      <td>-0.32671</td>\n",
              "      <td>0.24982</td>\n",
              "      <td>-0.27804</td>\n",
              "      <td>-0.99877</td>\n",
              "      <td>-0.39367</td>\n",
              "      <td>-0.30087</td>\n",
              "      <td>-0.24623</td>\n",
              "      <td>0.006483</td>\n",
              "      <td>-0.21982</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 104 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      token  complexity  word_length  ...       97        98       99\n",
              "0     river    0.000000            5  ... -0.69940  1.050600  0.24051\n",
              "1  brothers    0.000000            8  ... -0.24623  0.006483 -0.21982\n",
              "2  brothers    0.050000            8  ... -0.24623  0.006483 -0.21982\n",
              "3  brothers    0.150000            8  ... -0.24623  0.006483 -0.21982\n",
              "4  brothers    0.263889            8  ... -0.24623  0.006483 -0.21982\n",
              "\n",
              "[5 rows x 104 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCNNFO_uLuVz"
      },
      "source": [
        "# get data for training\n",
        "X_train = train_df_combined.drop(columns=['token', 'complexity'])\n",
        "Y_train = train_df_combined['complexity']"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzmE0l_emB6S"
      },
      "source": [
        "# train linear regression\n",
        "lr = LinearRegression().fit(X_train, Y_train)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luTw7T0HmB4B"
      },
      "source": [
        "# predict\n",
        "Y_pred = lr.predict(X_train)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHqtrYN0mB1q",
        "outputId": "39a37f97-de43-4626-ed3a-f9f1e8db16ff"
      },
      "source": [
        "# train loss (average absolute loss)\n",
        "num = len(Y_pred)\n",
        "losses = []\n",
        "for i in range(num):\n",
        "  loss = abs(Y_pred[i] - Y_train[i])\n",
        "  losses.append(loss)\n",
        "abl = sum(losses) / num\n",
        "print(\"average training absolute loss is \" + str(abl))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average training absolute loss is 0.07246931733686796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "q98DV01SovTY",
        "outputId": "314629c6-d91a-4a3a-c318-ec94d1294d6a"
      },
      "source": [
        "# on test\n",
        "test_tokens = test['token'].dropna().to_list()\n",
        "test_tokens = [token.lower() for token in test_tokens] # lowercase\n",
        "print(len(test_tokens))\n",
        "\n",
        "# create a dataframe for linear regression\n",
        "test_df = pd.DataFrame(test_tokens, columns =['token'])\n",
        "\n",
        "# add back complexity\n",
        "test_df['complexity'] = test['complexity']\n",
        "\n",
        "# word length\n",
        "test_df['word_length'] = test_df['token'].map(lambda x: len(x))\n",
        "\n",
        "# word frequency\n",
        "# tokenize the whole curpus\n",
        "temp = test['sentence'].to_list()\n",
        "texts = []\n",
        "for sent in temp:\n",
        "  sent = sent.lower()\n",
        "  sent = ''.join([c for c in sent if c not in punc])\n",
        "  words = [word for word in sent.split(' ') if (word.isalpha() and word not in stop_words)]\n",
        "  texts += words\n",
        "# count frequency\n",
        "count = Counter(texts)\n",
        "test_df['word_frequency'] = test_df['token'].map(lambda x: count[x])\n",
        "\n",
        "# create the weight matrix\n",
        "weight_matrix = create_weights_matrix(test_tokens)\n",
        "print(weight_matrix.shape)\n",
        "\n",
        "# combine\n",
        "weight_matrix_df = pd.DataFrame(weight_matrix)\n",
        "test_df_combined = pd.concat([test_df, weight_matrix_df], axis=1)\n",
        "test_df_combined.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "917\n",
            "(917, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>word_length</th>\n",
              "      <th>word_frequency</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hand</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>-0.24945</td>\n",
              "      <td>0.37033</td>\n",
              "      <td>-0.058334</td>\n",
              "      <td>-0.25367</td>\n",
              "      <td>0.18709</td>\n",
              "      <td>0.81760</td>\n",
              "      <td>-0.045494</td>\n",
              "      <td>0.072066</td>\n",
              "      <td>-0.059079</td>\n",
              "      <td>-0.053018</td>\n",
              "      <td>-0.15681</td>\n",
              "      <td>-0.18621</td>\n",
              "      <td>0.786770</td>\n",
              "      <td>0.56263</td>\n",
              "      <td>0.023693</td>\n",
              "      <td>0.241160</td>\n",
              "      <td>0.034775</td>\n",
              "      <td>0.117630</td>\n",
              "      <td>-0.15757</td>\n",
              "      <td>-0.39749</td>\n",
              "      <td>0.210680</td>\n",
              "      <td>-0.14618</td>\n",
              "      <td>0.014017</td>\n",
              "      <td>-0.22373</td>\n",
              "      <td>0.54225</td>\n",
              "      <td>0.47379</td>\n",
              "      <td>-0.62683</td>\n",
              "      <td>-0.38803</td>\n",
              "      <td>0.27510</td>\n",
              "      <td>-0.54687</td>\n",
              "      <td>0.49211</td>\n",
              "      <td>0.052715</td>\n",
              "      <td>-0.12911</td>\n",
              "      <td>0.2554</td>\n",
              "      <td>-0.005657</td>\n",
              "      <td>-0.194310</td>\n",
              "      <td>...</td>\n",
              "      <td>0.64518</td>\n",
              "      <td>0.76559</td>\n",
              "      <td>-0.22193</td>\n",
              "      <td>0.39305</td>\n",
              "      <td>0.13373</td>\n",
              "      <td>-0.17641</td>\n",
              "      <td>0.36222</td>\n",
              "      <td>0.47786</td>\n",
              "      <td>-0.43591</td>\n",
              "      <td>-0.13363</td>\n",
              "      <td>-0.13145</td>\n",
              "      <td>0.206730</td>\n",
              "      <td>0.37353</td>\n",
              "      <td>-0.70188</td>\n",
              "      <td>0.53225</td>\n",
              "      <td>0.103710</td>\n",
              "      <td>-0.70940</td>\n",
              "      <td>0.24331</td>\n",
              "      <td>-0.15523</td>\n",
              "      <td>0.20785</td>\n",
              "      <td>1.19970</td>\n",
              "      <td>-0.036297</td>\n",
              "      <td>-0.79044</td>\n",
              "      <td>-0.27794</td>\n",
              "      <td>-1.40760</td>\n",
              "      <td>-0.363180</td>\n",
              "      <td>0.40219</td>\n",
              "      <td>0.17401</td>\n",
              "      <td>-0.080981</td>\n",
              "      <td>-0.40688</td>\n",
              "      <td>-0.044007</td>\n",
              "      <td>-0.14964</td>\n",
              "      <td>0.39369</td>\n",
              "      <td>-0.014732</td>\n",
              "      <td>-0.41309</td>\n",
              "      <td>-0.061931</td>\n",
              "      <td>-0.088387</td>\n",
              "      <td>-0.230930</td>\n",
              "      <td>0.93931</td>\n",
              "      <td>0.091475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hand</td>\n",
              "      <td>0.197368</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>-0.24945</td>\n",
              "      <td>0.37033</td>\n",
              "      <td>-0.058334</td>\n",
              "      <td>-0.25367</td>\n",
              "      <td>0.18709</td>\n",
              "      <td>0.81760</td>\n",
              "      <td>-0.045494</td>\n",
              "      <td>0.072066</td>\n",
              "      <td>-0.059079</td>\n",
              "      <td>-0.053018</td>\n",
              "      <td>-0.15681</td>\n",
              "      <td>-0.18621</td>\n",
              "      <td>0.786770</td>\n",
              "      <td>0.56263</td>\n",
              "      <td>0.023693</td>\n",
              "      <td>0.241160</td>\n",
              "      <td>0.034775</td>\n",
              "      <td>0.117630</td>\n",
              "      <td>-0.15757</td>\n",
              "      <td>-0.39749</td>\n",
              "      <td>0.210680</td>\n",
              "      <td>-0.14618</td>\n",
              "      <td>0.014017</td>\n",
              "      <td>-0.22373</td>\n",
              "      <td>0.54225</td>\n",
              "      <td>0.47379</td>\n",
              "      <td>-0.62683</td>\n",
              "      <td>-0.38803</td>\n",
              "      <td>0.27510</td>\n",
              "      <td>-0.54687</td>\n",
              "      <td>0.49211</td>\n",
              "      <td>0.052715</td>\n",
              "      <td>-0.12911</td>\n",
              "      <td>0.2554</td>\n",
              "      <td>-0.005657</td>\n",
              "      <td>-0.194310</td>\n",
              "      <td>...</td>\n",
              "      <td>0.64518</td>\n",
              "      <td>0.76559</td>\n",
              "      <td>-0.22193</td>\n",
              "      <td>0.39305</td>\n",
              "      <td>0.13373</td>\n",
              "      <td>-0.17641</td>\n",
              "      <td>0.36222</td>\n",
              "      <td>0.47786</td>\n",
              "      <td>-0.43591</td>\n",
              "      <td>-0.13363</td>\n",
              "      <td>-0.13145</td>\n",
              "      <td>0.206730</td>\n",
              "      <td>0.37353</td>\n",
              "      <td>-0.70188</td>\n",
              "      <td>0.53225</td>\n",
              "      <td>0.103710</td>\n",
              "      <td>-0.70940</td>\n",
              "      <td>0.24331</td>\n",
              "      <td>-0.15523</td>\n",
              "      <td>0.20785</td>\n",
              "      <td>1.19970</td>\n",
              "      <td>-0.036297</td>\n",
              "      <td>-0.79044</td>\n",
              "      <td>-0.27794</td>\n",
              "      <td>-1.40760</td>\n",
              "      <td>-0.363180</td>\n",
              "      <td>0.40219</td>\n",
              "      <td>0.17401</td>\n",
              "      <td>-0.080981</td>\n",
              "      <td>-0.40688</td>\n",
              "      <td>-0.044007</td>\n",
              "      <td>-0.14964</td>\n",
              "      <td>0.39369</td>\n",
              "      <td>-0.014732</td>\n",
              "      <td>-0.41309</td>\n",
              "      <td>-0.061931</td>\n",
              "      <td>-0.088387</td>\n",
              "      <td>-0.230930</td>\n",
              "      <td>0.93931</td>\n",
              "      <td>0.091475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hand</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>-0.24945</td>\n",
              "      <td>0.37033</td>\n",
              "      <td>-0.058334</td>\n",
              "      <td>-0.25367</td>\n",
              "      <td>0.18709</td>\n",
              "      <td>0.81760</td>\n",
              "      <td>-0.045494</td>\n",
              "      <td>0.072066</td>\n",
              "      <td>-0.059079</td>\n",
              "      <td>-0.053018</td>\n",
              "      <td>-0.15681</td>\n",
              "      <td>-0.18621</td>\n",
              "      <td>0.786770</td>\n",
              "      <td>0.56263</td>\n",
              "      <td>0.023693</td>\n",
              "      <td>0.241160</td>\n",
              "      <td>0.034775</td>\n",
              "      <td>0.117630</td>\n",
              "      <td>-0.15757</td>\n",
              "      <td>-0.39749</td>\n",
              "      <td>0.210680</td>\n",
              "      <td>-0.14618</td>\n",
              "      <td>0.014017</td>\n",
              "      <td>-0.22373</td>\n",
              "      <td>0.54225</td>\n",
              "      <td>0.47379</td>\n",
              "      <td>-0.62683</td>\n",
              "      <td>-0.38803</td>\n",
              "      <td>0.27510</td>\n",
              "      <td>-0.54687</td>\n",
              "      <td>0.49211</td>\n",
              "      <td>0.052715</td>\n",
              "      <td>-0.12911</td>\n",
              "      <td>0.2554</td>\n",
              "      <td>-0.005657</td>\n",
              "      <td>-0.194310</td>\n",
              "      <td>...</td>\n",
              "      <td>0.64518</td>\n",
              "      <td>0.76559</td>\n",
              "      <td>-0.22193</td>\n",
              "      <td>0.39305</td>\n",
              "      <td>0.13373</td>\n",
              "      <td>-0.17641</td>\n",
              "      <td>0.36222</td>\n",
              "      <td>0.47786</td>\n",
              "      <td>-0.43591</td>\n",
              "      <td>-0.13363</td>\n",
              "      <td>-0.13145</td>\n",
              "      <td>0.206730</td>\n",
              "      <td>0.37353</td>\n",
              "      <td>-0.70188</td>\n",
              "      <td>0.53225</td>\n",
              "      <td>0.103710</td>\n",
              "      <td>-0.70940</td>\n",
              "      <td>0.24331</td>\n",
              "      <td>-0.15523</td>\n",
              "      <td>0.20785</td>\n",
              "      <td>1.19970</td>\n",
              "      <td>-0.036297</td>\n",
              "      <td>-0.79044</td>\n",
              "      <td>-0.27794</td>\n",
              "      <td>-1.40760</td>\n",
              "      <td>-0.363180</td>\n",
              "      <td>0.40219</td>\n",
              "      <td>0.17401</td>\n",
              "      <td>-0.080981</td>\n",
              "      <td>-0.40688</td>\n",
              "      <td>-0.044007</td>\n",
              "      <td>-0.14964</td>\n",
              "      <td>0.39369</td>\n",
              "      <td>-0.014732</td>\n",
              "      <td>-0.41309</td>\n",
              "      <td>-0.061931</td>\n",
              "      <td>-0.088387</td>\n",
              "      <td>-0.230930</td>\n",
              "      <td>0.93931</td>\n",
              "      <td>0.091475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hand</td>\n",
              "      <td>0.267857</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>-0.24945</td>\n",
              "      <td>0.37033</td>\n",
              "      <td>-0.058334</td>\n",
              "      <td>-0.25367</td>\n",
              "      <td>0.18709</td>\n",
              "      <td>0.81760</td>\n",
              "      <td>-0.045494</td>\n",
              "      <td>0.072066</td>\n",
              "      <td>-0.059079</td>\n",
              "      <td>-0.053018</td>\n",
              "      <td>-0.15681</td>\n",
              "      <td>-0.18621</td>\n",
              "      <td>0.786770</td>\n",
              "      <td>0.56263</td>\n",
              "      <td>0.023693</td>\n",
              "      <td>0.241160</td>\n",
              "      <td>0.034775</td>\n",
              "      <td>0.117630</td>\n",
              "      <td>-0.15757</td>\n",
              "      <td>-0.39749</td>\n",
              "      <td>0.210680</td>\n",
              "      <td>-0.14618</td>\n",
              "      <td>0.014017</td>\n",
              "      <td>-0.22373</td>\n",
              "      <td>0.54225</td>\n",
              "      <td>0.47379</td>\n",
              "      <td>-0.62683</td>\n",
              "      <td>-0.38803</td>\n",
              "      <td>0.27510</td>\n",
              "      <td>-0.54687</td>\n",
              "      <td>0.49211</td>\n",
              "      <td>0.052715</td>\n",
              "      <td>-0.12911</td>\n",
              "      <td>0.2554</td>\n",
              "      <td>-0.005657</td>\n",
              "      <td>-0.194310</td>\n",
              "      <td>...</td>\n",
              "      <td>0.64518</td>\n",
              "      <td>0.76559</td>\n",
              "      <td>-0.22193</td>\n",
              "      <td>0.39305</td>\n",
              "      <td>0.13373</td>\n",
              "      <td>-0.17641</td>\n",
              "      <td>0.36222</td>\n",
              "      <td>0.47786</td>\n",
              "      <td>-0.43591</td>\n",
              "      <td>-0.13363</td>\n",
              "      <td>-0.13145</td>\n",
              "      <td>0.206730</td>\n",
              "      <td>0.37353</td>\n",
              "      <td>-0.70188</td>\n",
              "      <td>0.53225</td>\n",
              "      <td>0.103710</td>\n",
              "      <td>-0.70940</td>\n",
              "      <td>0.24331</td>\n",
              "      <td>-0.15523</td>\n",
              "      <td>0.20785</td>\n",
              "      <td>1.19970</td>\n",
              "      <td>-0.036297</td>\n",
              "      <td>-0.79044</td>\n",
              "      <td>-0.27794</td>\n",
              "      <td>-1.40760</td>\n",
              "      <td>-0.363180</td>\n",
              "      <td>0.40219</td>\n",
              "      <td>0.17401</td>\n",
              "      <td>-0.080981</td>\n",
              "      <td>-0.40688</td>\n",
              "      <td>-0.044007</td>\n",
              "      <td>-0.14964</td>\n",
              "      <td>0.39369</td>\n",
              "      <td>-0.014732</td>\n",
              "      <td>-0.41309</td>\n",
              "      <td>-0.061931</td>\n",
              "      <td>-0.088387</td>\n",
              "      <td>-0.230930</td>\n",
              "      <td>0.93931</td>\n",
              "      <td>0.091475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>entrance</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>0.25776</td>\n",
              "      <td>0.10680</td>\n",
              "      <td>-0.162650</td>\n",
              "      <td>0.42335</td>\n",
              "      <td>0.19078</td>\n",
              "      <td>0.46283</td>\n",
              "      <td>-0.959150</td>\n",
              "      <td>0.931740</td>\n",
              "      <td>0.471610</td>\n",
              "      <td>0.390770</td>\n",
              "      <td>0.54734</td>\n",
              "      <td>0.41967</td>\n",
              "      <td>0.086822</td>\n",
              "      <td>0.53954</td>\n",
              "      <td>0.354970</td>\n",
              "      <td>-0.028346</td>\n",
              "      <td>0.427080</td>\n",
              "      <td>0.036569</td>\n",
              "      <td>-0.49700</td>\n",
              "      <td>-0.49543</td>\n",
              "      <td>-0.031232</td>\n",
              "      <td>-0.30298</td>\n",
              "      <td>-0.417180</td>\n",
              "      <td>-0.78459</td>\n",
              "      <td>0.70473</td>\n",
              "      <td>-0.59741</td>\n",
              "      <td>-0.33173</td>\n",
              "      <td>-0.38813</td>\n",
              "      <td>0.17189</td>\n",
              "      <td>-0.78565</td>\n",
              "      <td>-0.17219</td>\n",
              "      <td>-0.140190</td>\n",
              "      <td>0.61492</td>\n",
              "      <td>0.5713</td>\n",
              "      <td>0.751090</td>\n",
              "      <td>-0.015942</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.60393</td>\n",
              "      <td>0.47454</td>\n",
              "      <td>0.80912</td>\n",
              "      <td>0.81709</td>\n",
              "      <td>-0.12876</td>\n",
              "      <td>-0.39310</td>\n",
              "      <td>0.17656</td>\n",
              "      <td>-0.29797</td>\n",
              "      <td>-0.32614</td>\n",
              "      <td>-0.26522</td>\n",
              "      <td>-0.37006</td>\n",
              "      <td>-0.016956</td>\n",
              "      <td>0.92268</td>\n",
              "      <td>-0.71606</td>\n",
              "      <td>-0.38524</td>\n",
              "      <td>-0.085737</td>\n",
              "      <td>0.68111</td>\n",
              "      <td>0.32080</td>\n",
              "      <td>0.45870</td>\n",
              "      <td>-0.82737</td>\n",
              "      <td>0.22932</td>\n",
              "      <td>0.314500</td>\n",
              "      <td>-0.21221</td>\n",
              "      <td>-0.65293</td>\n",
              "      <td>-0.31427</td>\n",
              "      <td>-0.037493</td>\n",
              "      <td>0.16126</td>\n",
              "      <td>-0.46719</td>\n",
              "      <td>0.630660</td>\n",
              "      <td>0.26426</td>\n",
              "      <td>0.527780</td>\n",
              "      <td>-0.34505</td>\n",
              "      <td>0.06620</td>\n",
              "      <td>0.722400</td>\n",
              "      <td>-0.11057</td>\n",
              "      <td>-0.005771</td>\n",
              "      <td>-0.059336</td>\n",
              "      <td>0.013272</td>\n",
              "      <td>0.97305</td>\n",
              "      <td>0.454050</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 104 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      token  complexity  word_length  ...        97       98        99\n",
              "0      hand    0.000000            4  ... -0.230930  0.93931  0.091475\n",
              "1      hand    0.197368            4  ... -0.230930  0.93931  0.091475\n",
              "2      hand    0.200000            4  ... -0.230930  0.93931  0.091475\n",
              "3      hand    0.267857            4  ... -0.230930  0.93931  0.091475\n",
              "4  entrance    0.000000            8  ...  0.013272  0.97305  0.454050\n",
              "\n",
              "[5 rows x 104 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEy_z413ovRJ",
        "outputId": "46ac635a-3036-4842-9d9d-b3744aab81f6"
      },
      "source": [
        "# get data for test\n",
        "X_test = test_df_combined.drop(columns=['token', 'complexity'])\n",
        "Y_test = test_df_combined['complexity']\n",
        "\n",
        "# predict\n",
        "Y_pred = lr.predict(X_test)\n",
        "\n",
        "# test loss (average absolute loss)\n",
        "num = len(Y_pred)\n",
        "losses = []\n",
        "for i in range(num):\n",
        "  loss = abs(Y_pred[i] - Y_test[i])\n",
        "  losses.append(loss)\n",
        "abl = sum(losses) / num\n",
        "print(\"average test absolute loss is \" + str(abl))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average test absolute loss is 0.07283375821746224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6xzRuFTovN5"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un9WFQkFUQcx"
      },
      "source": [
        "LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "hDibiYSCLuf1",
        "outputId": "8923709a-6f34-4334-e52b-e6197e1744b6"
      },
      "source": [
        "# tokenize sentences\n",
        "\n",
        "def tokenize(sent, token, punc, stop_words):\n",
        "  \"\"\" lowercase, padded, remove stopwords and punctuations \"\"\"\n",
        "  # lowercase\n",
        "  sent = sent.lower()\n",
        "  # remove punctuation and stopwords\n",
        "  sent = ''.join([c for c in sent if c not in punc]) \n",
        "  tokens = [word for word in sent.split(' ') if (word.isalpha() and word not in stop_words)]\n",
        "  # pad\n",
        "  tokens.insert(0, '<s>')\n",
        "  tokens.append('</s>')\n",
        "  # pad the token with special symbols\n",
        "  for i in range(len(tokens)):\n",
        "    if tokens[i] == token:\n",
        "      tokens.insert(i, '_START')\n",
        "      tokens.insert(i+2, '_END')\n",
        "      break\n",
        "\n",
        "  return tokens\n",
        "\n",
        "def preprocess(df):\n",
        "  data = df[['sentence', 'token', 'complexity']]\n",
        "  data['tokenized_sentence'] = data.apply(lambda row: tokenize(row['sentence'], row['token'], punc, stop_words), axis=1)\n",
        "  data = data.drop(columns=['sentence'])\n",
        "  return data\n",
        "\n",
        "train_data = preprocess(df)\n",
        "test_data = preprocess(test)\n",
        "\n",
        "train_data.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>tokenized_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>river</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[&lt;s&gt;, behold, came, _START, river, _END, seven, cattle, sleek, fat, fed, marsh, grass, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[&lt;s&gt;, fellow, bondservant, _START, brothers, _END, prophets, keep, words, book, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>[&lt;s&gt;, man, lord, land, said, us, know, honest, men, leave, one, _START, brothers, _END, take, grain, famine, houses, go, way, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>[&lt;s&gt;, shimei, sixteen, sons, six, daughters, _START, brothers, _END, didnt, many, children, neither, family, multiply, like, children, judah, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.263889</td>\n",
              "      <td>[&lt;s&gt;, put, _START, brothers, _END, far, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      token  ...                                                                                                                                   tokenized_sentence\n",
              "0     river  ...                                                         [<s>, behold, came, _START, river, _END, seven, cattle, sleek, fat, fed, marsh, grass, </s>]\n",
              "1  brothers  ...                                                                [<s>, fellow, bondservant, _START, brothers, _END, prophets, keep, words, book, </s>]\n",
              "2  brothers  ...                  [<s>, man, lord, land, said, us, know, honest, men, leave, one, _START, brothers, _END, take, grain, famine, houses, go, way, </s>]\n",
              "3  brothers  ...  [<s>, shimei, sixteen, sons, six, daughters, _START, brothers, _END, didnt, many, children, neither, family, multiply, like, children, judah, </s>]\n",
              "4  brothers  ...                                                                                                        [<s>, put, _START, brothers, _END, far, </s>]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7zco41RQrgD"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSGGceDJQo-0"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "l4fnXnBekvW3",
        "outputId": "5114cecc-6da5-44e3-f2fd-0a1deb55cc1a"
      },
      "source": [
        "# convert words to index for training and testing purpose\n",
        "sentences = train_data['tokenized_sentence'].to_list()\n",
        "temp = []\n",
        "for sent in sentences:\n",
        "  temp += sent\n",
        "temp = set(temp)\n",
        "# for words that are unknown\n",
        "temp.add('_UNKNOWN') \n",
        "temp.add('_PADDING')\n",
        "print(len(temp))\n",
        "\n",
        "# need to pad sentences to the same length\n",
        "lengths = [len(sent) for sent in sentences]\n",
        "pad_length = max(lengths)\n",
        "print(pad_length)\n",
        "\n",
        "# construct dictionaries\n",
        "word2index = {}\n",
        "index2word = {}\n",
        "for i, word in enumerate(temp):\n",
        "  word2index[word] = i\n",
        "  index2word[i] = word\n",
        "\n",
        "def word_to_index(sentence):\n",
        "  # sentence: a list of strings\n",
        "  r = []\n",
        "  for word in sentence:\n",
        "    if word in word2index:\n",
        "      r.append(word2index[word])\n",
        "    else:\n",
        "      r.append(word2index['_UNKNOWN'])\n",
        "  diff = pad_length - len(sentence)\n",
        "  pad_index = word2index['_PADDING']\n",
        "  for i in range(diff):\n",
        "    r.append(pad_index)\n",
        "  return r\n",
        "\n",
        "train_data['number_sentence'] = train_data['tokenized_sentence'].map(lambda sent: word_to_index(sent))\n",
        "test_data['number_sentence'] = test_data['tokenized_sentence'].map(lambda sent: word_to_index(sent))\n",
        "\n",
        "train_data = train_data.drop(columns=['tokenized_sentence'])\n",
        "test_data = test_data.drop(columns=['tokenized_sentence'])\n",
        "\n",
        "train_data.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14826\n",
            "118\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>number_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>river</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[13409, 4292, 10528, 10347, 6537, 9638, 712, 9660, 13303, 10478, 5150, 6242, 2529, 2644, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[13409, 13627, 10664, 10347, 13026, 9638, 7290, 11929, 10967, 10746, 2644, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>[13409, 1167, 12437, 4974, 5651, 1133, 13173, 3449, 11862, 3240, 12701, 10347, 13026, 9638, 14735, 9937, 6319, 12505, 2733, 12430, 2644, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>[13409, 11540, 197, 6182, 3090, 11841, 10347, 13026, 9638, 10114, 7761, 2755, 5011, 984, 12263, 3339, 2755, 5205, 2644, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.263889</td>\n",
              "      <td>[13409, 3947, 10347, 13026, 9638, 11103, 2644, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, ...]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      token  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          number_sentence\n",
              "0     river  ...        [13409, 4292, 10528, 10347, 6537, 9638, 712, 9660, 13303, 10478, 5150, 6242, 2529, 2644, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, ...]\n",
              "1  brothers  ...    [13409, 13627, 10664, 10347, 13026, 9638, 7290, 11929, 10967, 10746, 2644, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, ...]\n",
              "2  brothers  ...  [13409, 1167, 12437, 4974, 5651, 1133, 13173, 3449, 11862, 3240, 12701, 10347, 13026, 9638, 14735, 9937, 6319, 12505, 2733, 12430, 2644, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, ...]\n",
              "3  brothers  ...       [13409, 11540, 197, 6182, 3090, 11841, 10347, 13026, 9638, 10114, 7761, 2755, 5011, 984, 12263, 3339, 2755, 5205, 2644, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, ...]\n",
              "4  brothers  ...        [13409, 3947, 10347, 13026, 9638, 11103, 2644, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, 7815, ...]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNw9bjg2Luda",
        "outputId": "b13e9768-f429-4bdd-8aa1-bb7677ac2b5d"
      },
      "source": [
        "# do a simple check\n",
        "print(df.shape)\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)\n",
        "print(len(word2index.keys()))\n",
        "print(len(index2word.keys()))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7662, 5)\n",
            "(7662, 3)\n",
            "(917, 3)\n",
            "14826\n",
            "14826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL3ASkWeeGZo",
        "outputId": "c8708bd7-36dc-4362-be83-3a1c24ef4a65"
      },
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOTh_SM0eGXC"
      },
      "source": [
        "train_sentences = np.array(train_data['number_sentence'].to_list())\n",
        "train_labels = np.array(train_data['complexity'].to_list())\n",
        "test_sentences = np.array(test_data['number_sentence'].to_list())\n",
        "test_labels = np.array(test_data['complexity'].to_list())\n",
        "\n",
        "training = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n",
        "testing = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(training, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "test_loader = DataLoader(testing, shuffle=True, batch_size=batch_size, drop_last=True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmDPzyUOeGUo"
      },
      "source": [
        "# the LSTM class\n",
        "class ComplexityNet(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        super(ComplexityNet, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        #self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)       \n",
        "        out = self.fc(lstm_out[:, -1, :])      \n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        return hidden"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFAZtgekeGSh",
        "outputId": "d502771a-52ce-4ed3-dc20-23abb1c17a31"
      },
      "source": [
        "# some parameters\n",
        "vocab_size = len(word2index) + 1\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "\n",
        "model = ComplexityNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "model.to(device)\n",
        "print(model)\n",
        "\n",
        "lr=0.01\n",
        "criterion = nn.L1Loss(reduction='mean')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "epochs = 100\n",
        "counter = 0\n",
        "print_every = 256\n",
        "clip = 5"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ComplexityNet(\n",
            "  (embedding): Embedding(14827, 400)\n",
            "  (lstm): LSTM(400, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eggcDcSeGQJ",
        "outputId": "c7c8c474-4757-4011-d606-f7bfd2b479d8"
      },
      "source": [
        "# training\n",
        "model.train()\n",
        "for i in range(epochs):\n",
        "  h = model.init_hidden(batch_size)\n",
        "    \n",
        "  for inputs, labels in train_loader:\n",
        "    counter += 1\n",
        "    h = tuple([e.data for e in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    model.zero_grad()\n",
        "    output, h = model(inputs, h)\n",
        "    # cross entropy for multiple classes\n",
        "    # output is of shape 64 * 5 while labels is of shape 64\n",
        "    loss = criterion(output, labels) \n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    if counter%print_every == 0:\n",
        "      print(\"Epoch: {}/{}...\".format(i+1, epochs), \"Step: {}...\".format(counter), \"Loss: {:.6f}...\".format(loss.item()))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 5/100... Step: 256... Loss: 0.111072...\n",
            "Epoch: 9/100... Step: 512... Loss: 0.120014...\n",
            "Epoch: 14/100... Step: 768... Loss: 0.128292...\n",
            "Epoch: 18/100... Step: 1024... Loss: 0.107732...\n",
            "Epoch: 22/100... Step: 1280... Loss: 0.089548...\n",
            "Epoch: 27/100... Step: 1536... Loss: 0.116203...\n",
            "Epoch: 31/100... Step: 1792... Loss: 0.117202...\n",
            "Epoch: 35/100... Step: 2048... Loss: 0.109630...\n",
            "Epoch: 40/100... Step: 2304... Loss: 0.106438...\n",
            "Epoch: 44/100... Step: 2560... Loss: 0.093245...\n",
            "Epoch: 48/100... Step: 2816... Loss: 0.086738...\n",
            "Epoch: 53/100... Step: 3072... Loss: 0.097762...\n",
            "Epoch: 57/100... Step: 3328... Loss: 0.101385...\n",
            "Epoch: 61/100... Step: 3584... Loss: 0.105967...\n",
            "Epoch: 66/100... Step: 3840... Loss: 0.102142...\n",
            "Epoch: 70/100... Step: 4096... Loss: 0.103135...\n",
            "Epoch: 74/100... Step: 4352... Loss: 0.091270...\n",
            "Epoch: 79/100... Step: 4608... Loss: 0.107417...\n",
            "Epoch: 83/100... Step: 4864... Loss: 0.109148...\n",
            "Epoch: 87/100... Step: 5120... Loss: 0.104011...\n",
            "Epoch: 92/100... Step: 5376... Loss: 0.114962...\n",
            "Epoch: 96/100... Step: 5632... Loss: 0.106310...\n",
            "Epoch: 100/100... Step: 5888... Loss: 0.108885...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1997iM_psXaP"
      },
      "source": [
        "# save the model\n",
        "torch.save(model.state_dict(), \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/LSTM_0.pt\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASr80p94sXPA",
        "outputId": "5b322565-3293-4050-894b-0906b1d08c4c"
      },
      "source": [
        "test_losses = []\n",
        "num_correct = 0\n",
        "h = model.init_hidden(batch_size)\n",
        "\n",
        "model.eval()\n",
        "for inputs, labels in test_loader:\n",
        "    \n",
        "    h = tuple([each.data for each in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    output, h = model(inputs, h)\n",
        "    fetched_labels = labels.data.tolist()\n",
        "    fetched_output = output.squeeze().data.tolist()\n",
        "    #print(len(fetched_labels)) # 32\n",
        "    #print(len(fetched_output[0])) # 5\n",
        "\n",
        "    # absolute mean average; different from the cross entropy used in training\n",
        "    denominator = len(output)\n",
        "    for i in range(denominator):\n",
        "      diff = abs( fetched_labels[i] - fetched_output[i] )\n",
        "      if diff <= 0.05: # a difference that is small\n",
        "        num_correct += 1\n",
        "      test_losses.append(diff)\n",
        "\n",
        "print(test_losses)        \n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.12320338884989424, 0.14716172218322754, 0.007352166705661356, 0.005368039721534351, 0.024393865040370444, 0.02471327781677246, 0.06903672218322754, 0.05065436924205102, 0.006536722183227539, 0.11846327781677246, 0.017900358546863893, 0.08466172218322754, 0.11764783329433876, 0.03431449996100533, 0.07158827781677246, 0.14542561107211632, 0.012694047047541651, 0.06209227773878312, 0.16903672218322752, 0.039870055516560865, 0.12418378100675698, 0.03778672218322754, 0.06903672218322754, 0.009088277816772461, 0.19346327781677247, 0.08153672218322755, 0.5343144999610052, 0.06699268958147836, 0.07598116662767196, 0.01132042067391531, 0.006536722183227539, 0.018463277816772455, 0.13153672218322754, 0.2696946169200697, 0.039870055516560865, 0.04601040639375387, 0.02216172218322754, 0.10880944945595483, 0.04601040639375387, 0.1388896633596981, 0.5065367221832275, 0.06012994448343914, 0.07598116662767196, 0.28987005551656086, 0.009088277816772461, 0.09477201630087462, 0.05228680722853718, 0.04033827781677246, 0.08274899210248676, 0.006536722183227539, 0.4815367221832275, 0.1898700555165609, 0.09477201630087462, 0.12418378100675698, 0.20653672218322755, 0.24337882744638534, 0.08721327781677246, 0.006536722183227539, 0.02287504252265482, 0.25653672218322754, 0.026796611150105787, 0.007352166705661356, 0.035129944483439174, 0.1363204206739153, 0.03758092487559597, 0.06209227773878312, 0.09653672218322756, 0.12495777481480652, 0.04820338884989417, 0.08274899210248676, 0.11846327781677246, 0.15359554571263934, 0.24346327781677246, 0.04609485676414085, 0.09068550003899467, 0.09582243646894184, 0.30199126763777295, 0.13153672218322754, 0.32796529361179894, 0.19403672218322754, 0.019779067290456698, 0.04703470638820101, 0.030346245992751375, 0.300654369242051, 0.24264783329433876, 0.021242604536168663, 0.023203388849894258, 0.04033827781677246, 0.02917756353105816, 0.007352166705661356, 0.05596327781677246, 0.09403672218322756, 0.28778672218322754, 0.08274899210248676, 0.08466172218322754, 0.23022093270954336, 0.009088277816772461, 0.05065436924205102, 0.08274899210248676, 0.039870055516560865, 0.006536722183227539, 0.006536722183227539, 0.09640445428736072, 0.12418378100675698, 0.07232619586743805, 0.02124105559455025, 0.03431449996100533, 0.09346327781677244, 0.07796529361179894, 0.08153672218322755, 0.25653672218322754, 0.01132042067391531, 0.300654369242051, 0.01132042067391531, 0.22082243646894184, 0.08987005551656091, 0.02216172218322754, 0.20653672218322755, 0.11367957932608463, 0.06846327781677247, 0.28431449996100533, 0.008169160169713668, 0.09477201630087462, 0.019779067290456698, 0.039870055516560865, 0.10653672218322752, 0.3940367221832275, 0.06290772226121685, 0.16830142806558046, 0.11846327781677246, 0.006536722183227539, 0.02917756353105816, 0.07232619586743805, 0.04601040639375387, 0.09744581309231842, 0.026796611150105787, 0.12581621899324302, 0.006536722183227539, 0.32796529361179894, 0.024393865040370444, 0.006536722183227539, 0.023203388849894258, 0.16278672218322754, 0.3940367221832275, 0.006536722183227539, 0.007352166705661356, 0.02917756353105816, 0.006536722183227539, 0.10028672218322754, 0.006536722183227539, 0.28153672218322756, 0.05596327781677246, 0.05916830113059601, 0.37192133756784296, 0.07679661115010578, 0.01969461692006963, 0.04609485676414085, 0.06290772226121685, 0.006536722183227539, 0.005963277816772472, 0.1830073104185217, 0.11846327781677246, 0.02287504252265482, 0.07241064623782512, 0.006621172553614552, 0.10060613495962961, 0.03153672218322756, 0.16278672218322754, 0.006536722183227539, 0.05596327781677246, 0.0633549040014093, 0.20296529361179894, 0.05199126763777295, 0.06903672218322754, 0.3398700555165609, 0.02287504252265482, 0.006621172553614552, 0.30341172218322754, 0.006621172553614552, 0.035948486889109954, 0.0854840906042802, 0.006536722183227539, 0.05228680722853718, 0.005963277816772472, 0.006621172553614552, 0.04703470638820101, 0.03758092487559597, 0.04601040639375387, 0.04225100789751324, 0.007352166705661356, 0.14346327781677248, 0.0048269141804088145, 0.056536722183227583, 0.28431449996100533, 0.04820338884989417, 0.12017308581959124, 0.06903672218322754, 0.31010815075465614, 0.10457438892788357, 0.10060613495962961, 0.010129944483439152, 0.02737005551656091, 0.02287504252265482, 0.04346327781677245, 0.006536722183227539, 0.08274899210248676, 0.21241907512440406, 0.010129944483439152, 0.05596327781677246, 0.08006613394793338, 0.05028145963495431, 0.15359554571263934, 0.09864198534112223, 0.09477201630087462, 0.18510815075465614, 0.006536722183227539, 0.28778672218322754, 0.09346327781677244, 0.08987005551656091, 0.06536025159499226, 0.45966172218322754, 0.3565367221832275, 0.38153672218322754, 0.02471327781677246, 0.02287504252265482, 0.024393865040370444, 0.04346327781677245, 0.07232619586743805, 0.03192481627831087, 0.017900358546863893, 0.09068550003899467, 0.09403672218322756, 0.02124105559455025, 0.06209227773878312, 0.15417756353105816, 0.05596327781677246, 0.10947789865381574, 0.06699268958147836, 0.03293696202729879, 0.02471327781677246, 0.05596327781677246, 0.06846327781677247, 0.15931449996100533, 0.07241064623782512, 0.056536722183227583, 0.02216172218322754, 0.294998260644766, 0.2704256110721164, 0.29225100789751324, 0.02216172218322754, 0.16012994448343915, 0.02287504252265482, 0.10375894440544975, 0.09477201630087462, 0.007352166705661356, 0.05199126763777295, 0.006536722183227539, 0.02124105559455025, 0.06012994448343914, 0.06290772226121685, 0.04820338884989417, 0.07679661115010578, 0.09477201630087462, 0.12581621899324302, 0.02471327781677246, 0.006536722183227539, 0.04108232543582005, 0.039870055516560865, 0.11846327781677246, 0.15359554571263934, 0.05596327781677246, 0.10653672218322752, 0.11846327781677246, 0.021242604536168663, 0.3232033888498942, 0.03192481627831087, 0.006536722183227539, 0.06699268958147836, 0.007352166705661356, 0.04225100789751324, 0.27216172218322754, 0.08987005551656091, 0.16012994448343915, 0.07679661115010578, 0.06536025159499226, 0.006536722183227539, 0.28778672218322754, 0.25653672218322754, 0.294998260644766, 0.04403672218322752, 0.026796611150105787, 0.006536722183227539, 0.08169857193441954, 0.006536722183227539, 0.021242604536168663, 0.06012994448343914, 0.023203388849894258, 0.010129944483439152, 0.04346327781677245, 0.18709227773878312, 0.09640445428736072, 0.03431449996100533, 0.10269056833707363, 0.026796611150105787, 0.08096327781677248, 0.11764783329433876, 0.09477201630087462, 0.09346327781677244, 0.04703470638820101, 0.27124260453616866, 0.10177481742132277, 0.08169857193441954, 0.1883549040014093, 0.006536722183227539, 0.023203388849894258, 0.08466172218322754, 0.05228680722853718, 0.08274899210248676, 0.04346327781677245, 0.13153672218322754, 0.2232033888498942, 0.021242604536168663, 0.006536722183227539, 0.02042561107211638, 0.27439386504037044, 0.1767966111501058, 0.06012994448343914, 0.14716172218322754, 0.07232619586743805, 0.04033827781677246, 0.07679661115010578, 0.16278672218322754, 0.10653672218322752, 0.04820338884989417, 0.08721327781677246, 0.06846327781677247, 0.006536722183227539, 0.11367957932608463, 0.06209227773878312, 0.22528672218322754, 0.13153672218322754, 0.1111103366403019, 0.056536722183227583, 0.02124105559455025, 0.04346327781677245, 0.21568550003899467, 0.07158827781677246, 0.02471327781677246, 0.029263994910500246, 0.02287504252265482, 0.04601040639375387, 0.026796611150105787, 0.010129944483439152, 0.08006613394793338, 0.20653672218322755, 0.5065367221832275, 0.05341172218322754, 0.1732033888498941, 0.14346327781677248, 0.06846327781677247, 0.38153672218322754, 0.20653672218322755, 0.24091172218322754, 0.04346327781677245, 0.10283827781677246, 0.10947789865381574, 0.10947789865381574, 0.02124105559455025, 0.07320338884989419, 0.006536722183227539, 0.02917756353105816, 0.3108845482701841, 0.04033827781677246, 0.16443145902533285, 0.11367957932608463, 0.22392802653105365, 0.1656276312741366, 0.005368039721534351, 0.02287504252265482, 0.06536025159499226, 0.06489184924534391, 0.09477201630087462, 0.10060613495962961, 0.08153672218322755, 0.24346327781677246, 0.28987005551656086, 0.04033827781677246, 0.13235216670566136, 0.07598116662767196, 0.08721327781677246, 0.1898700555165609, 0.08721327781677246, 0.07796529361179894, 0.34864198534112223, 0.03778672218322754, 0.023203388849894258, 0.14716172218322754, 0.11367957932608463, 0.24183083983028641, 0.026796611150105787, 0.09477201630087462, 0.11846327781677246, 0.16443145902533285, 0.035948486889109954, 0.054155769802275266, 0.035129944483439174, 0.2696946169200697, 0.02917756353105816, 0.02287504252265482, 0.11591172218322754, 0.04703470638820101, 0.22875894440544975, 0.10375894440544975, 0.24091172218322754, 0.05341172218322754, 0.026796611150105787, 0.1767966111501058, 0.16846327781677245, 0.2690367221832275, 0.018463277816772455, 0.03153672218322756, 0.1101299444834391, 0.05596327781677246, 0.10653672218322752, 0.006536722183227539, 0.1898918492453439, 0.02124105559455025, 0.024393865040370444, 0.03758092487559597, 0.25653672218322754, 0.16846327781677245, 0.01969461692006963, 0.06903672218322754, 0.03285251165691172, 0.006536722183227539, 0.05065436924205102, 0.08153672218322755, 0.12017308581959124, 0.024393865040370444, 0.006536722183227539, 0.006536722183227539, 0.056536722183227583, 0.026796611150105787, 0.02124105559455025, 0.009088277816772461, 0.05199126763777295, 0.13811566955164856, 0.054155769802275266, 0.08345979910630447, 0.27320338884989426, 0.3300661339479334, 0.04703470638820101, 0.11846327781677246, 0.04403672218322752, 0.11846327781677246, 0.03293696202729879, 0.09346327781677244, 0.03758092487559597, 0.27124260453616866, 0.006536722183227539, 0.04609485676414085, 0.22875894440544975, 0.11764783329433876, 0.012694047047541651, 0.01132042067391531, 0.14784107000931446, 0.15359554571263934, 0.06699268958147836, 0.006536722183227539, 0.28153672218322756, 0.11764783329433876, 0.03758092487559597, 0.28431449996100533, 0.03758092487559597, 0.11903672218322753, 0.039870055516560865, 0.030963277816772466, 0.19346327781677247, 0.01969461692006963, 0.02216172218322754, 0.006536722183227539, 0.04033827781677246, 0.07320338884989419, 0.06903672218322754, 0.24566715696583624, 0.13153672218322754, 0.07320338884989419, 0.18510815075465614, 0.12017308581959124, 0.15653672218322756, 0.009088277816772461, 0.07679661115010578, 0.04033827781677246, 0.006536722183227539, 0.010129944483439152, 0.27216172218322754, 0.11591172218322754, 0.10653672218322752, 0.010129944483439152, 0.02042561107211638, 0.006536722183227539, 0.08006613394793338, 0.006536722183227539, 0.006536722183227539, 0.14346327781677248, 0.006536722183227539, 0.16830142806558046, 0.028275852618010133, 0.026796611150105787, 0.07158827781677246, 0.04601040639375387, 0.06489184924534391, 0.03293696202729879, 0.02124105559455025, 0.12320338884989424, 0.10283827781677246, 0.026796611150105787, 0.05916830113059601, 0.007352166705661356, 0.009088277816772461, 0.006536722183227539, 0.04703470638820101, 0.19403672218322754, 0.1363204206739153, 0.2232033888498942, 0.018463277816772455, 0.04346327781677245, 0.07300873236222699, 0.06536025159499226, 0.02042561107211638, 0.2232033888498942, 0.16278672218322754, 0.10653672218322752, 0.024393865040370444, 0.12679661115010582, 0.026796611150105787, 0.21241907512440406, 0.06209227773878312, 0.03778672218322754, 0.16830142806558046, 0.08721327781677246, 0.09346327781677244, 0.04225100789751324, 0.12495777481480652, 0.10436280913974921, 0.05596327781677246, 0.04820338884989417, 0.02471327781677246, 0.06012994448343914, 0.08006613394793338, 0.007352166705661356, 0.005963277816772472, 0.13987005551656084, 0.13153672218322754, 0.01132042067391531, 0.06903672218322754, 0.07320338884989419, 0.14403672218322755, 0.03778672218322754, 0.14346327781677248, 0.47875894440544975, 0.07598116662767196, 0.03431449996100533, 0.08987005551656091, 0.006536722183227539, 0.16846327781677245, 0.06010815075465614, 0.03431449996100533, 0.006536722183227539, 0.005963277816772472, 0.10375894440544975, 0.17841172218322754, 0.23153672218322752, 0.27216172218322754, 0.35177481742132277, 0.021242604536168663, 0.1280786624321571, 0.0854840906042802, 0.009088277816772461, 0.08153672218322755, 0.3398700555165609, 0.14939386504037044, 0.07796529361179894, 0.10653672218322752, 0.04609485676414085, 0.023203388849894258, 0.024393865040370444, 0.06903672218322754, 0.07158827781677246, 0.11591172218322754, 0.06846327781677247, 0.06903672218322754, 0.06903672218322754, 0.056536722183227583, 0.10375894440544975, 0.07320338884989419, 0.07320338884989419, 0.31536025159499226, 0.06012994448343914, 0.006536722183227539, 0.19346327781677247, 0.3315367221832275, 0.006536722183227539, 0.11846327781677246, 0.05925275150098297, 0.11846327781677246, 0.06010815075465614, 0.05341172218322754, 0.11764783329433876, 0.05341172218322754, 0.01132042067391531, 0.3676478332943387, 0.0184414840879894, 0.05341172218322754, 0.18510815075465614, 0.17841172218322754, 0.31903672218322754, 0.05925275150098297, 0.030346245992751375, 0.056536722183227583, 0.02124105559455025, 0.05115558550908017, 0.03778672218322754, 0.07796529361179894, 0.02287504252265482, 0.02471327781677246, 0.10457438892788357, 0.006536722183227539, 0.13153672218322754, 0.10653672218322752, 0.08721327781677246, 0.30341172218322754, 0.03431449996100533, 0.06012994448343914, 0.12320338884989424, 0.13929661115010578, 0.10028672218322754, 0.10177481742132277, 0.02287504252265482, 0.19346327781677247, 0.1732033888498941, 0.17841172218322754, 0.4315367221832276, 0.48867957932608463, 0.05596327781677246, 0.04703470638820101, 0.11764783329433876, 0.08987005551656091, 0.008169160169713668, 0.02124105559455025, 0.09477201630087462, 0.19346327781677247, 0.026796611150105787, 0.006536722183227539, 0.22875894440544975, 0.006536722183227539, 0.45098116662767196, 0.02124105559455025, 0.008169160169713668, 0.008169160169713668, 0.026071973468946352, 0.0855685409746672, 0.05596327781677246, 0.007352166705661356, 0.07598116662767196, 0.009088277816772461, 0.02471327781677246, 0.006621172553614552, 0.05341172218322754, 0.03431449996100533, 0.12320338884989424, 0.25653672218322754, 0.06209227773878312, 0.08653672218322755, 0.02287504252265482, 0.006536722183227539, 0.11367957932608463, 0.04108232543582005, 0.27216172218322754, 0.056536722183227583, 0.39939386504037044, 0.03153672218322756, 0.06536025159499226, 0.05862005551656091, 0.06209227773878312, 0.08987005551656091, 0.08006613394793338, 0.13153672218322754, 0.05341172218322754, 0.19346327781677247, 0.13987005551656084, 0.01132042067391531, 0.006536722183227539, 0.007352166705661356, 0.22392802653105365, 0.007352166705661356, 0.1830073104185217, 0.1830073104185217, 0.14346327781677248, 0.09477201630087462, 0.06536025159499226, 0.06209227773878312, 0.14346327781677248, 0.035948486889109954, 0.09582243646894184, 0.06012994448343914, 0.006536722183227539, 0.16846327781677245, 0.04346327781677245, 0.06489184924534391, 0.006536722183227539, 0.09068550003899467, 0.03758092487559597, 0.16846327781677245, 0.06010815075465614, 0.24183083983028641, 0.0184414840879894, 0.03153672218322756, 0.28987005551656086, 0.035948486889109954, 0.11764783329433876, 0.02124105559455025, 0.1388896633596981, 0.006536722183227539, 0.34582243646894184, 0.07158827781677246, 0.20966172218322754, 0.030963277816772466, 0.12418378100675698, 0.22082243646894184, 0.24346327781677246, 0.20098116662767196, 0.06489184924534391, 0.03285251165691172, 0.06606053170703707, 0.04346327781677245, 0.08721327781677246, 0.0184414840879894, 0.10375894440544975, 0.25653672218322754, 0.06846327781677247, 0.10028672218322754, 0.006621172553614552, 0.13235216670566136, 0.20653672218322755, 0.15417756353105816, 0.11764783329433876, 0.19403672218322754, 0.05916830113059601, 0.026796611150105787, 0.09640445428736072, 0.06606053170703707, 0.03778672218322754, 0.03778672218322754, 0.04703470638820101, 0.06012994448343914, 0.18153672218322753, 0.22082243646894184, 0.04609485676414085, 0.38153672218322754, 0.1388896633596981, 0.3232033888498942, 0.20390514323585907, 0.28778672218322754, 0.10028672218322754, 0.02471327781677246, 0.06012994448343914, 0.02216172218322754, 0.03285251165691172, 0.056536722183227583, 0.009088277816772461, 0.17841172218322754, 0.04901883337232807, 0.07320338884989419, 0.056536722183227583, 0.03153672218322756, 0.09346327781677244, 0.09068550003899467, 0.04225100789751324, 0.2232033888498942, 0.1732033888498941, 0.16278672218322754, 0.15653672218322756, 0.5065367221832275, 0.13153672218322754, 0.04346327781677245, 0.19771319277146282, 0.035948486889109954, 0.07598116662767196, 0.02917756353105816, 0.006536722183227539, 0.16278672218322754, 0.07175411348757532, 0.25653672218322754, 0.04033827781677246, 0.07384441449091983, 0.03431449996100533, 0.056536722183227583, 0.06422902987553525, 0.04901883337232807, 0.12320338884989424, 0.006536722183227539, 0.05596327781677246, 0.12418378100675698, 0.10028672218322754, 0.10177481742132277, 0.03153672218322756, 0.04609485676414085, 0.06536025159499226, 0.05065436924205102, 0.3315367221832275, 0.08987005551656091, 0.035948486889109954, 0.05596327781677246, 0.23987005551656082, 0.2232033888498942, 0.24346327781677246, 0.02471327781677246, 0.13153672218322754, 0.006536722183227539, 0.01969461692006963, 0.20296529361179894, 0.05065436924205102, 0.08466172218322754, 0.08987005551656091, 0.07471854036504577, 0.29225100789751324, 0.04346327781677245, 0.20966172218322754, 0.08987005551656091, 0.06489184924534391, 0.10269056833707363, 0.15653672218322756, 0.06903672218322754, 0.006536722183227539, 0.04609485676414085, 0.05228680722853718, 0.2089176745641798, 0.07679661115010578, 0.10060613495962961, 0.15653672218322756, 0.1732033888498941, 0.08608217672868212, 0.24264783329433876, 0.25653672218322754, 0.03758092487559597, 0.05925275150098297, 0.04901883337232807, 0.04901883337232807, 0.09349324392235803, 0.03758092487559597, 0.32598116662767196, 0.07598116662767196, 0.06209227773878312, 0.02216172218322754, 0.07384441449091983, 0.4330073104185217, 0.06010815075465614, 0.017272801626296325, 0.06699268958147836, 0.21241907512440406, 0.007352166705661356, 0.1405221013461842, 0.20653672218322755, 0.005963277816772472, 0.1388896633596981, 0.01903672218322755, 0.006536722183227539, 0.21568550003899467, 0.18709227773878312, 0.36591172218322754, 0.04403672218322752, 0.13153672218322754, 0.13153672218322754, 0.12017308581959124, 0.06012994448343914, 0.02917756353105816, 0.3447720163008746, 0.33466172218322754, 0.021242604536168663, 0.42320338884989417, 0.02471327781677246, 0.11846327781677246, 0.1101299444834391, 0.05596327781677246, 0.09640445428736072, 0.23987005551656082, 0.08987005551656091, 0.04346327781677245, 0.02917756353105816, 0.06489184924534391, 0.04403672218322752, 0.10947789865381574, 0.11846327781677246, 0.10653672218322752, 0.009088277816772461]\n",
            "Test loss: 0.100\n",
            "Test accuracy: 36.423%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w52OdjZtBS_"
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29TcMd795Kyg"
      },
      "source": [
        "LSTM with GLove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXb6AzSzSYJT"
      },
      "source": [
        "# restart the loader\n",
        "train_sentences = np.array(train_data['number_sentence'].to_list())\n",
        "train_labels = np.array(train_data['complexity'].to_list())\n",
        "test_sentences = np.array(test_data['number_sentence'].to_list())\n",
        "test_labels = np.array(test_data['complexity'].to_list())\n",
        "\n",
        "training = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n",
        "testing = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(training, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "test_loader = DataLoader(testing, shuffle=True, batch_size=batch_size, drop_last=True)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zi2FuT5tBQn"
      },
      "source": [
        "def create_weights_matrix_tensor(target_vocab, dimension=100):\n",
        "  \"\"\" create a matrix containing vectors for each word in glove \"\"\"\n",
        "  matrix_len = len(target_vocab)\n",
        "  weights_matrix = np.zeros((matrix_len, dimension))\n",
        "\n",
        "  for i, word in enumerate(target_vocab):\n",
        "      try: \n",
        "          weights_matrix[i] = glove[word]\n",
        "      except KeyError:\n",
        "          weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, )) # initialize a random vector\n",
        "  return torch.from_numpy(weights_matrix) # must be a tensor!!\n",
        "\n",
        "def create_emb_layer(weights_matrix, non_trainable=False):\n",
        "  \"\"\" an embedding layer \"\"\"\n",
        "  num_embeddings, embedding_dim = weights_matrix.size()\n",
        "  emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "  emb_layer.load_state_dict({'weight': weights_matrix})\n",
        "  if non_trainable:\n",
        "    emb_layer.weight.requires_grad = False\n",
        "\n",
        "  return emb_layer, num_embeddings, embedding_dim"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3EQOW-g5LON"
      },
      "source": [
        "# The model; inherits from the previous model\n",
        "class ComplexityNetGlove(ComplexityNet):\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, weights_matrix, drop_prob=0.5):\n",
        "        super(ComplexityNetGlove, self).__init__(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True) # use the Glove\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        #self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyFDmkvv5pQ1",
        "outputId": "7f87a685-3eef-4945-8dbf-f783d2382dc6"
      },
      "source": [
        "vocab_size = len(word2index) + 1\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "weights_matrix = create_weights_matrix_tensor(word2index.keys())\n",
        "\n",
        "model = ComplexityNetGlove(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, weights_matrix)\n",
        "model.to(device)\n",
        "print(model)\n",
        "\n",
        "lr=0.01\n",
        "criterion = nn.L1Loss(reduction='mean')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "epochs = 100\n",
        "counter = 0\n",
        "print_every = 256\n",
        "clip = 5"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ComplexityNetGlove(\n",
            "  (embedding): Embedding(14826, 100)\n",
            "  (lstm): LSTM(100, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5F9qH-4v5pGn",
        "outputId": "2ac76b2f-9f57-4e2d-defc-3c5d21db7e33"
      },
      "source": [
        "# training\n",
        "model.train()\n",
        "for i in range(epochs):\n",
        "  h = model.init_hidden(batch_size)\n",
        "    \n",
        "  for inputs, labels in train_loader:\n",
        "    counter += 1\n",
        "    h = tuple([e.data for e in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    model.zero_grad()\n",
        "    output, h = model(inputs, h)\n",
        "    # cross entropy for multiple classes\n",
        "    # output is of shape 64 * 5 while labels is of shape 64\n",
        "    loss = criterion(output, labels) \n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    if counter%print_every == 0:\n",
        "      print(\"Epoch: {}/{}...\".format(i+1, epochs), \"Step: {}...\".format(counter), \"Loss: {:.6f}...\".format(loss.item()))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 5/100... Step: 256... Loss: 0.093580...\n",
            "Epoch: 9/100... Step: 512... Loss: 0.119150...\n",
            "Epoch: 14/100... Step: 768... Loss: 0.101767...\n",
            "Epoch: 18/100... Step: 1024... Loss: 0.102664...\n",
            "Epoch: 22/100... Step: 1280... Loss: 0.116188...\n",
            "Epoch: 27/100... Step: 1536... Loss: 0.102156...\n",
            "Epoch: 31/100... Step: 1792... Loss: 0.106548...\n",
            "Epoch: 35/100... Step: 2048... Loss: 0.103106...\n",
            "Epoch: 40/100... Step: 2304... Loss: 0.111678...\n",
            "Epoch: 44/100... Step: 2560... Loss: 0.109645...\n",
            "Epoch: 48/100... Step: 2816... Loss: 0.113371...\n",
            "Epoch: 53/100... Step: 3072... Loss: 0.108595...\n",
            "Epoch: 57/100... Step: 3328... Loss: 0.098336...\n",
            "Epoch: 61/100... Step: 3584... Loss: 0.109610...\n",
            "Epoch: 66/100... Step: 3840... Loss: 0.112114...\n",
            "Epoch: 70/100... Step: 4096... Loss: 0.118717...\n",
            "Epoch: 74/100... Step: 4352... Loss: 0.097163...\n",
            "Epoch: 79/100... Step: 4608... Loss: 0.098568...\n",
            "Epoch: 83/100... Step: 4864... Loss: 0.099111...\n",
            "Epoch: 87/100... Step: 5120... Loss: 0.103717...\n",
            "Epoch: 92/100... Step: 5376... Loss: 0.101651...\n",
            "Epoch: 96/100... Step: 5632... Loss: 0.104506...\n",
            "Epoch: 100/100... Step: 5888... Loss: 0.094930...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOadn6Uv5o_G"
      },
      "source": [
        "# save the model\n",
        "torch.save(model.state_dict(), \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/LSTM_Glove.pt\")"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPj_sgcM7oJ2",
        "outputId": "b51c12ff-6f8f-4af3-976d-ce2917b95af1"
      },
      "source": [
        "test_losses = []\n",
        "num_correct = 0\n",
        "h = model.init_hidden(batch_size)\n",
        "\n",
        "model.eval()\n",
        "for inputs, labels in test_loader:\n",
        "    \n",
        "    h = tuple([each.data for each in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    output, h = model(inputs, h)\n",
        "    fetched_labels = labels.data.tolist()\n",
        "    fetched_output = output.squeeze().data.tolist()\n",
        "    #print(len(fetched_labels)) # 32\n",
        "    #print(len(fetched_output[0])) # 5\n",
        "\n",
        "    # absolute mean average\n",
        "    denominator = len(output)\n",
        "    for i in range(denominator):\n",
        "      diff = abs( fetched_labels[i] - fetched_output[i] )\n",
        "      if diff <= 0.05: # a difference that is small\n",
        "        num_correct += 1\n",
        "      test_losses.append(diff)\n",
        "\n",
        "print(test_losses)        \n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.33405596017837524, 0.040944039821624756, 0.04729125429602232, 0.1438852758968578, 0.14655590057373047, 0.07219406962394714, 0.04094401001930237, 0.07219409942626953, 0.054101934558466847, 0.05761070648829145, 0.05280590057373047, 0.05761070648829145, 0.0052297541073390574, 0.00761070648829143, 0.0014703556110984284, 0.09094403982162474, 0.04094401001930237, 0.029039277916862893, 0.08506168688044827, 0.14094403982162473, 0.11906903982162476, 0.13469403982162476, 0.05905598998069761, 0.08405596017837524, 0.08506174648509304, 0.07572256724039717, 0.0007226268450418738, 0.1812780929936303, 0.09063490754679626, 0.07670292959493752, 0.1355265484136694, 0.03405590057373048, 0.24094403982162477, 0.05483292871051365, 0.08856314704531712, 0.07016710109180885, 0.06619881732123234, 0.00761070648829143, 0.07670301900190468, 0.034055960178375255, 0.06132862784645776, 0.16218090057373047, 0.0031735178302315648, 0.040944069623947144, 0.10344401001930237, 0.03718098998069763, 0.04094409942626953, 0.042389293511708614, 0.056569039821624756, 0.051161223336269934, 0.08405596017837524, 0.06594403982162475, 0.003444039821624778, 0.08041778363679586, 0.018579710097540003, 0.25593096017837524, 0.15548447200230187, 0.07665832553591045, 0.19818630555401678, 0.12048955397172406, 0.017879489590139963, 0.019204968991486937, 0.009055989980697676, 0.07940557828316316, 0.13093096017837524, 0.034055960178375255, 0.19094403982162478, 0.025319069623947144, 0.11530596017837524, 0.18405596017837522, 0.03048456140926903, 0.15023243076661053, 0.19094403982162478, 0.013166232241524578, 0.025722537438074733, 0.08405593037605286, 0.43405590057373045, 0.07665832553591045, 0.07665829573358807, 0.0703558343298295, 0.009694069623947144, 0.024845433862585753, 0.06872181759940255, 0.08405596017837524, 0.021555960178375244, 0.25317360723719873, 0.027055150932735916, 0.35191310303551815, 0.11594403982162477, 0.1943501076277565, 0.017879489590139963, 0.005930989980697632, 0.05905590057373045, 0.13022975410733906, 0.11906909942626953, 0.0285003152158525, 0.05880124228341238, 0.02572259704271951, 0.09357561876899315, 0.028500404622819664, 0.10761070648829144, 0.11530596017837524, 0.009055900573730513, 0.14094403982162473, 0.040944039821624756, 0.02308689696448185, 0.09451552799769808, 0.027786145084782665, 0.22294484906726408, 0.042389293511708614, 0.11530596017837524, 0.07665832553591045, 0.025722626845041896, 0.03405590057373048, 0.23683367835150826, 0.16594403982162476, 0.08781906962394714, 0.01713451601210092, 0.05627821220291984, 0.06872178779708016, 0.040944039821624756, 0.025722626845041896, 0.018579710097540003, 0.27155596017837524, 0.12917930413694945, 0.20905596017837524, 0.10344403982162476, 0.2840559601783752, 0.04094427824020386, 0.013166500462426067, 0.08781927824020386, 0.19094427824020388, 0.04094427824020386, 0.20761094490687054, 0.0804179326484078, 0.005930721759796143, 0.012627150331224746, 0.04094427824020386, 0.09976780765196858, 0.11038872268464825, 0.10905572175979616, 0.09649983379575947, 0.02427761157353714, 0.09357585718757225, 0.05410217297704595, 0.07427761157353718, 0.43127794398201835, 0.09649983379575947, 0.06872205601798165, 0.032585133524501986, 0.10344424843788147, 0.13816650046242607, 0.20905572175979614, 0.24030572175979614, 0.13469427824020386, 0.02484519544400665, 0.026238395887262733, 0.04238905509312951, 0.45905572175979614, 0.26787925117156086, 0.14655572175979614, 0.2215557217597961, 0.2531733688186196, 0.04094427824020386, 0.07035604294608622, 0.21594427824020385, 0.023087135383060953, 0.03597879868287307, 0.005930721759796143, 0.05230791460384021, 0.26316650046242607, 0.06017504747097305, 0.20905572175979614, 0.30116098491769083, 0.06619857890265324, 0.20905572175979614, 0.0380030901808488, 0.028500166204240562, 0.10761094490687054, 0.13469427824020386, 0.04094427824020386, 0.1838014210973467, 0.0007223884264627722, 0.16594427824020386, 0.09357585718757225, 0.23683349953757393, 0.16594427824020386, 0.03718072175979614, 0.023087135383060953, 0.24477000747408184, 0.12427761157353717, 0.0945157068116324, 0.17334143604551044, 0.19238905509312942, 0.22468072175979614, 0.01821700551293115, 0.1330495413980986, 0.07665856395448956, 0.08405575156211853, 0.009694278240203857, 0.15905572175979615, 0.03718072175979614, 0.08781927824020386, 0.13762715033122475, 0.05880142109734671, 0.13022999252591816, 0.048341436045510444, 0.04094427824020386, 0.08405572175979614, 0.07155572175979613, 0.18127794398201835, 0.0076109449068705315, 0.07427761157353718, 0.05656927824020386, 0.13762715033122475, 0.025722388426462794, 0.05483316712909275, 0.2373728496687753, 0.055650160593145065, 0.05905572175979612, 0.04238905509312951, 0.11237284966877531, 0.15905572175979615, 0.10344427824020386, 0.009055721759796187, 0.06843072175979614, 0.05284904014496575, 0.011532513534321442, 0.09649983379575947, 0.07016683287090736, 0.07269208539615984, 0.08781927824020386, 0.07035604294608622, 0.1355263099950903, 0.16493807470097266, 0.08405572175979614, 0.05280572175979614, 0.06619857890265324, 0.29094427824020386, 0.005930721759796143, 0.19094427824020388, 0.016748029452103852, 0.07035604294608622, 0.13816650046242607, 0.029580641876567504, 0.40350016620424056, 0.07035604294608622, 0.05410217297704595, 0.07035604294608622, 0.04729101587744322, 0.05761094490687055, 0.029039516335441995, 0.013166500462426067, 0.26262715033122475, 0.24030572175979614, 0.01787925117156086, 0.027786383503361767, 0.19343072175979614, 0.014611277315351723, 0.11989164666125651, 0.046555721759796165, 0.21594427824020385, 0.21594427824020385, 0.01787925117156086, 0.04094427824020386, 0.16493807470097266, 0.08781927824020386, 0.07269208539615984, 0.011532513534321442, 0.0945157068116324, 0.16594427824020386, 0.04238905509312951, 0.2840557217597961, 0.04729101587744322, 0.025319278240203857, 0.3201668328709073, 0.11594427824020387, 0.22376160411273727, 0.07670278058332558, 0.14388545471079212, 0.012627150331224746, 0.16594427824020386, 0.07427761157353718, 0.11038872268464825, 0.02484519544400665, 0.15205538935131496, 0.05520956791364223, 0.09094427824020385, 0.011687300707164616, 0.026238395887262733, 0.04729101587744322, 0.31843072175979614, 0.005930721759796143, 0.33405572175979614, 0.07747677439137513, 0.09094427824020385, 0.19343072175979614, 0.05280572175979614, 0.04094427824020386, 0.13469427824020386, 0.11447369000490976, 0.17780572175979614, 0.07219427824020386, 0.12082042764214906, 0.04094427824020386, 0.05483316712909275, 0.06843072175979614, 0.021555721759796143, 0.05483316712909275, 0.026238395887262733, 0.13093072175979614, 0.32444033714441156, 0.08506192529902737, 0.29094427824020386, 0.14094427824020384, 0.28048429318836754, 0.08041796245073018, 0.05483316712909275, 0.05344427824020387, 0.11237284966877531, 0.11237284966877531, 0.10761094490687054, 0.3465557217597961, 0.05483316712909275, 0.00147059402967753, 0.04094427824020386, 0.05280572175979614, 0.07572238842646284, 0.009055721759796187, 0.15031927824020386, 0.10761094490687054, 0.2757223884264628, 0.33405572175979614, 0.05761094490687055, 0.04094427824020386, 0.2222136164966383, 0.16594427824020386, 0.0007223884264627722, 0.13022999252591816, 0.20905572175979614, 0.08405572175979614, 0.17329721941667442, 0.10905572175979616, 0.03048429318836754, 0.026238395887262733, 0.08405572175979614, 0.021555721759796143, 0.04094427824020386, 0.08781927824020386, 0.1585913370637333, 0.17329721941667442, 0.0031733688186196263, 0.08506192529902737, 0.0034442782402038796, 0.04729101587744322, 0.05688180871631782, 0.025722388426462794, 0.028444278240203846, 0.04729101587744322, 0.11989164666125651, 0.24238905509312947, 0.04094427824020386, 0.12082042764214906, 0.12082042764214906, 0.014611277315351723, 0.07035604294608622, 0.1755596628555885, 0.04094427824020386, 0.08405572175979614, 0.06872205601798165, 0.01787925117156086, 0.08405572175979614, 0.009055721759796187, 0.07016683287090736, 0.07219427824020386, 0.04729101587744322, 0.1423890550931295, 0.10344427824020386, 0.07844427824020386, 0.07670278058332558, 0.24094427824020387, 0.11594427824020387, 0.021555721759796143, 0.021555721759796143, 0.0945157068116324, 0.014611277315351723, 0.2242776115735372, 0.11038872268464825, 0.021555721759796143, 0.10611454528920794, 0.023087135383060953, 0.014611277315351723, 0.09357585718757225, 0.07844427824020386, 0.07427761157353718, 0.10611454528920794, 0.41218072175979614, 0.11183349953757393, 0.07016683287090736, 0.015944278240203835, 0.04094427824020386, 0.061996898230384345, 0.11594427824020387, 0.00147059402967753, 0.025722388426462794, 0.13469427824020386, 0.12572238842646272, 0.07665856395448956, 0.026238395887262733, 0.07016683287090736, 0.011687300707164616, 0.22468072175979614, 0.09447238842646272, 0.07269208539615984, 0.06872205601798165, 0.148087135383061, 0.05656927824020386, 0.05905572175979612, 0.2757223884264628, 0.12427761157353717, 0.04094427824020386, 0.07670278058332558, 0.05520956791364223, 0.032585133524501986, 0.009055721759796187, 0.11906927824020386, 0.054293816997891375, 0.07427761157353718, 0.09357585718757225, 0.0914086629362667, 0.17334143604551044, 0.04094427824020386, 0.10611454528920794, 0.032585133524501986, 0.04094427824020386, 0.013166500462426067, 0.13961127731535172, 0.07035604294608622, 0.05656927824020386, 0.04729101587744322, 0.05284904014496575, 0.19516683287090736, 0.08405572175979614, 0.04094427824020386, 0.05880142109734671, 0.2972910158774432, 0.05905572175979612, 0.07427761157353718, 0.06872205601798165, 0.005229992525918159, 0.2923890550931295, 0.15905572175979615, 0.04094427824020386, 0.13469427824020386, 0.021555721759796143, 0.02427761157353714, 0.055650160593145065, 0.07035604294608622, 0.0031733688186196263, 0.07427761157353718, 0.09776246005838571, 0.24238905509312947, 0.10191286461693905, 0.20165856395448956, 0.009055721759796187, 0.014628488766519676, 0.00667476937884387, 0.005930721759796143, 0.026238395887262733, 0.04094427824020386, 0.12155572175979612, 0.021555721759796143, 0.15905572175979615, 0.10344427824020386, 0.11530572175979614, 0.09357585718757225, 0.05627794398201835, 0.05483316712909275, 0.10761094490687054, 0.08506192529902737, 0.0076109449068705315, 0.09094427824020385, 0.12572238842646272, 0.023087135383060953, 0.024273113064143925, 0.04094427824020386, 0.15905572175979615, 0.01787925117156086, 0.07219427824020386, 0.1181466308507052, 0.02484519544400665, 0.10344427824020386, 0.06872205601798165, 0.009694278240203857, 0.24030572175979614, 0.04094427824020386, 0.2242776115735372, 0.012627150331224746, 0.054293816997891375, 0.009694278240203857, 0.02427761157353714, 0.04238905509312951, 0.14094427824020384, 0.05656927824020386, 0.07355297389237775, 0.02427761157353714, 0.20905572175979614, 0.08261094490687057, 0.04094427824020386, 0.04094427824020386, 0.1423890550931295, 0.10344427824020386, 0.07572238842646284, 0.13022999252591816, 0.148087135383061, 0.06619857890265324, 0.11695045860190145, 0.025319278240203857, 0.07269208539615984, 0.05905572175979612, 0.10673375192441437, 0.014611277315351723, 0.16594427824020386, 0.028500166204240562, 0.021555721759796143, 0.24751726022133458, 0.04094427824020386, 0.08261094490687057, 0.148087135383061, 0.00147059402967753, 0.025319278240203857, 0.08506192529902737, 0.014628488766519676, 0.05656927824020386, 0.11530572175979614, 0.09094427824020385, 0.04729101587744322, 0.015944278240203835, 0.04094427824020386, 0.061996898230384345, 0.07219427824020386, 0.014628488766519676, 0.3855263099950903, 0.0672600677138881, 0.15205538935131496, 0.020110944906870487, 0.009694278240203857, 0.04094427824020386, 0.17572238842646282, 0.021555721759796143, 0.24751726022133458, 0.23683349953757393, 0.04905572175979617, 0.24094427824020387, 0.04094427824020386, 0.10761094490687054, 0.2634035478467527, 0.22468072175979614, 0.02427761157353714, 0.015944278240203835, 0.11237284966877531, 0.014611277315351723, 0.04094427824020386, 0.16594427824020386, 0.17334143604551044, 0.148087135383061, 0.09094427824020385, 0.026363414067488433, 0.16594427824020386, 0.0914086629362667, 0.10344427824020386, 0.011687300707164616, 0.05483316712909275, 0.025319278240203857, 0.10761094490687054, 0.09238905509312945, 0.13469427824020386, 0.20905572175979614, 0.0380030901808488, 0.18273993228611196, 0.10344427824020386, 0.19094427824020388, 0.02427761157353714, 0.06872205601798165, 0.09094427824020385, 0.04094427824020386, 0.04094427824020386, 0.11237284966877531, 0.04094427824020386, 0.15031927824020386, 0.29094427824020386, 0.14094427824020384, 0.20761094490687054, 0.10344427824020386, 0.021555721759796143, 0.013166500462426067, 0.16594427824020386, 0.17572238842646282, 0.1355263099950903, 0.004510267214341557, 0.11038872268464825, 0.04094427824020386, 0.20905572175979614, 0.13816650046242607, 0.12844427824020388, 0.027055389351315018, 0.05483316712909275, 0.023087135383060953, 0.1576109449068705, 0.07427761157353718, 0.03718072175979614, 0.45905572175979614, 0.22691286461693905, 0.10761094490687054, 0.11594427824020387, 0.11237284966877531, 0.014611277315351723, 0.07219427824020386, 0.005930721759796143, 0.44119857890265324, 0.04094427824020386, 0.04094427824020386, 0.28048429318836754, 0.029039516335441995, 0.011532513534321442, 0.04094427824020386, 0.12427761157353717, 0.18127794398201835, 0.05410217297704595, 0.05761094490687055, 0.08781927824020386, 0.025319278240203857, 0.0945157068116324, 0.09357585718757225, 0.29094427824020386, 0.014628488766519676, 0.10761094490687054, 0.12427761157353717, 0.07427761157353718, 0.14388545471079212, 0.11695045860190145, 0.011532513534321442, 0.04094427824020386, 0.009055721759796187, 0.3090557217597961, 0.09968072175979614, 0.05627794398201835, 0.45905572175979614, 0.17427761157353722, 0.0031733688186196263, 0.11447369000490976, 0.05627794398201835, 0.05483316712909275, 0.05880142109734671, 0.21594427824020385, 0.0914086629362667, 0.05483316712909275, 0.05344427824020387, 0.17983316712909275, 0.0672600677138881, 0.08506192529902737, 0.19516683287090736, 0.034055721759796154, 0.014611277315351723, 0.0007223884264627722, 0.25451026721434156, 0.015944278240203835, 0.06594427824020385, 0.12427761157353717, 0.23405572175979616, 0.07016683287090736, 0.06872205601798165, 0.07665856395448956, 0.16143667414074842, 0.07427761157353718, 0.07940581670174227, 0.04094427824020386, 0.005229992525918159, 0.08781927824020386, 0.029580641876567504, 0.011532513534321442, 0.04238905509312951, 0.054293816997891375, 0.04238905509312951, 0.026363414067488433, 0.05905572175979612, 0.0076109449068705315, 0.13093072175979614, 0.04094427824020386, 0.03718072175979614, 0.17572238842646282, 0.09794461064868493, 0.13961127731535172, 0.015944278240203835, 0.013166500462426067, 0.025722388426462794, 0.013166500462426067, 0.05905572175979612, 0.06017504747097305, 0.16594427824020386, 0.09968072175979614, 0.24094427824020387, 0.05880142109734671, 0.10611454528920794, 0.2923890550931295, 0.012627150331224746, 0.4868334995375738, 0.07572238842646284, 0.009055721759796187, 0.03905572175979616, 0.04094427824020386, 0.05880142109734671, 0.01787925117156086, 0.05483316712909275, 0.37572238842646277, 0.09238905509312945, 0.07665856395448956, 0.12917957235785094, 0.009055721759796187, 0.15548429318836754, 0.13762715033122475, 0.3465557217597961, 0.055650160593145065, 0.04729101587744322, 0.10344427824020386, 0.023087135383060953, 0.09094427824020385, 0.2222136164966383, 0.09649983379575947, 0.06872205601798165, 0.015944278240203835, 0.061996898230384345, 0.24094427824020387, 0.26316650046242607, 0.15642414281242767, 0.1576109449068705, 0.04094427824020386, 0.009694278240203857, 0.04094427824020386, 0.061996898230384345, 0.07035604294608622, 0.027237539941614375, 0.20165856395448956, 0.011139055093129513, 0.17983316712909275, 0.08261094490687057, 0.11447369000490976, 0.03860117630525073, 0.11447369000490976, 0.07427761157353718, 0.014611277315351723, 0.11530572175979614, 0.049964812668887026, 0.14655572175979614, 0.05656927824020386, 0.17780572175979614, 0.03048429318836754, 0.07572238842646284, 0.05410217297704595, 0.22468072175979614, 0.09094427824020385, 0.048341436045510444, 0.13022999252591816, 0.07747677439137513, 0.00667476937884387, 0.19589782702295394, 0.10673375192441437, 0.055650160593145065, 0.3042938169978914, 0.14094427824020384, 0.14388545471079212, 0.05905572175979612, 0.08506192529902737, 0.06872205601798165, 0.05280572175979614, 0.29834143604551044, 0.11906927824020386, 0.09976780765196858, 0.3840557217597962, 0.005930721759796143, 0.05880142109734671, 0.027055389351315018, 0.011532513534321442, 0.13469427824020386, 0.09655572175979615, 0.0034442782402038796, 0.05410217297704595, 0.10673375192441437, 0.05761094490687055, 0.09094427824020385, 0.11447369000490976, 0.07016683287090736, 0.04094427824020386, 0.05656927824020386, 0.10344427824020386, 0.032585133524501986, 0.06843072175979614, 0.04094427824020386, 0.06594427824020385, 0.22572238842646286, 0.16594427824020386, 0.07219427824020386, 0.0945157068116324, 0.12572238842646272, 0.17572238842646282, 0.0007223884264627722, 0.28718072175979614, 0.004510267214341557, 0.09863658593251157, 0.24094427824020387, 0.15905572175979615, 0.282585133524502, 0.009694278240203857, 0.0076109449068705315, 0.19434983940685502, 0.09976780765196858, 0.04094427824020386, 0.19238905509312942, 0.05656927824020386, 0.0945157068116324, 0.24477000747408184, 0.13816650046242607, 0.13405572175979613, 0.06619857890265324, 0.11237284966877531, 0.01587390357797791, 0.05656927824020386, 0.0076109449068705315, 0.12917957235785094, 0.07035604294608622, 0.24238905509312947, 0.0007223884264627722, 0.028500166204240562, 0.05905572175979612, 0.0945157068116324, 0.04094427824020386, 0.18677761157353717, 0.25593072175979614, 0.05656927824020386, 0.0034442782402038796, 0.028500166204240562, 0.00147059402967753, 0.24030572175979614, 0.0945157068116324, 0.005229992525918159, 0.1880031017696156, 0.11594427824020387, 0.10761094490687054, 0.09976780765196858, 0.1838014210973467, 0.04094427824020386, 0.08856332585925145, 0.05344427824020387, 0.1408739035779779, 0.07427761157353718, 0.17572238842646282, 0.21594427824020385, 0.03048429318836754, 0.06872205601798165, 0.07219427824020386, 0.04094427824020386, 0.09968072175979614, 0.13022999252591816, 0.09094427824020385, 0.00147059402967753, 0.046555721759796165, 0.027786383503361767, 0.04094427824020386, 0.12572238842646272, 0.027786383503361767, 0.12427761157353717, 0.04601224349892663, 0.004510267214341557, 0.00147059402967753, 0.12082042764214906, 0.05344427824020387, 0.0914086629362667, 0.0031733688186196263, 0.07427761157353718, 0.011532513534321442, 0.11906927824020386, 0.028500166204240562, 0.04729101587744322, 0.27850016620424056, 0.10761094490687054, 0.08506192529902737, 0.16218072175979614, 0.19094427824020388, 0.05410217297704595, 0.15350016620424056, 0.04238905509312951, 0.17644702610762225, 0.04238905509312951, 0.06475380204972772, 0.025319278240203857, 0.07035604294608622]\n",
            "Test loss: 0.097\n",
            "Test accuracy: 32.715%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}