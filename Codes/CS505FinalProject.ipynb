{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS505FinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK0gMeXPKEEX"
      },
      "source": [
        "import re\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from copy import deepcopy\n",
        "import math\n",
        "\n",
        "import torchtext.vocab\n",
        "\n",
        "import string\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import sklearn\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYtWoHTOKcNk",
        "outputId": "2ceed58e-9a92-4de9-84dc-606ca68224c5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNkKxGDiKcIN"
      },
      "source": [
        "# the paths; change when necessary\n",
        "TRAIN_RAW = \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/lcp_single_train.tsv\"\n",
        "TEST_RAW = \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/lcp_single_test.tsv\"\n",
        "TRAIN = \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/lcp_single_train_cleaned.tsv\"\n",
        "TEST = \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/lcp_single_test_cleaned.tsv\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVQ9mky8KcFb"
      },
      "source": [
        "# read the datasets\n",
        "# train\n",
        "with open(TRAIN_RAW, 'r') as f:\n",
        "  data = f.read()\n",
        "\n",
        "# need to remove \" from the string, otherwise parsing will have problems because some quotas are not closed \n",
        "data = data.replace('\"', '')\n",
        "\n",
        "with open(TRAIN, 'w') as f:\n",
        "  f.write(data)\n",
        "\n",
        "df = pd.read_csv(TRAIN, sep='\\t')\n",
        "\n",
        "# test\n",
        "with open(TEST_RAW, 'r') as f:\n",
        "  data = f.read()\n",
        "\n",
        "data = data.replace('\"', '')\n",
        "\n",
        "with open(TEST, 'w') as f:\n",
        "  f.write(data)\n",
        "\n",
        "test = pd.read_csv(TEST, sep='\\t')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Vw8Gk02zKcCc",
        "outputId": "587897be-6608-4fb1-bb14-affbbdc19de7"
      },
      "source": [
        "# take a look\n",
        "pd.set_option('display.max_colwidth', None) # show the whole sentence\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>corpus</th>\n",
              "      <th>sentence</th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3ZLW647WALVGE8EBR50EGUBPU4P32A</td>\n",
              "      <td>bible</td>\n",
              "      <td>Behold, there came up out of the river seven cattle, sleek and fat, and they fed in the marsh grass.</td>\n",
              "      <td>river</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>34R0BODSP1ZBN3DVY8J8XSIY551E5C</td>\n",
              "      <td>bible</td>\n",
              "      <td>I am a fellow bondservant with you and with your brothers, the prophets, and with those who keep the words of this book.</td>\n",
              "      <td>brothers</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3S1WOPCJFGTJU2SGNAN2Y213N6WJE3</td>\n",
              "      <td>bible</td>\n",
              "      <td>The man, the lord of the land, said to us, 'By this I will know that you are honest men: leave one of your brothers with me, and take grain for the famine of your houses, and go your way.</td>\n",
              "      <td>brothers</td>\n",
              "      <td>0.050000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3BFNCI9LYKQN09BHXHH9CLSX5KP738</td>\n",
              "      <td>bible</td>\n",
              "      <td>Shimei had sixteen sons and six daughters; but his brothers didn't have many children, neither did all their family multiply like the children of Judah.</td>\n",
              "      <td>brothers</td>\n",
              "      <td>0.150000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3G5RUKN2EC3YIWSKUXZ8ZVH95R49N2</td>\n",
              "      <td>bible</td>\n",
              "      <td>He has put my brothers far from me.</td>\n",
              "      <td>brothers</td>\n",
              "      <td>0.263889</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               id corpus  ...     token complexity\n",
              "0  3ZLW647WALVGE8EBR50EGUBPU4P32A  bible  ...     river   0.000000\n",
              "1  34R0BODSP1ZBN3DVY8J8XSIY551E5C  bible  ...  brothers   0.000000\n",
              "2  3S1WOPCJFGTJU2SGNAN2Y213N6WJE3  bible  ...  brothers   0.050000\n",
              "3  3BFNCI9LYKQN09BHXHH9CLSX5KP738  bible  ...  brothers   0.150000\n",
              "4  3G5RUKN2EC3YIWSKUXZ8ZVH95R49N2  bible  ...  brothers   0.263889\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "UTLwl2AWa-XG",
        "outputId": "f7e7f923-b979-4cef-e243-8f8af1011f88"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>corpus</th>\n",
              "      <th>sentence</th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3K8CQCU3KE19US5SN890DFPK3SANWR</td>\n",
              "      <td>bible</td>\n",
              "      <td>But he, beckoning to them with his hand to be silent, declared to them how the Lord had brought him out of the prison.</td>\n",
              "      <td>hand</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3Q2T3FD0ON86LCI41NJYV3PN0BW3MV</td>\n",
              "      <td>bible</td>\n",
              "      <td>If I forget you, Jerusalem, let my right hand forget its skill.</td>\n",
              "      <td>hand</td>\n",
              "      <td>0.197368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3ULIZ0H1VA5C32JJMKOTQ8Z4GUS51B</td>\n",
              "      <td>bible</td>\n",
              "      <td>the ten sons of Haman the son of Hammedatha, the Jew's enemy, but they didn't lay their hand on the plunder.</td>\n",
              "      <td>hand</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3BFF0DJK8XCEIOT30ZLBPPSRMZQTSD</td>\n",
              "      <td>bible</td>\n",
              "      <td>Let your hand be lifted up above your adversaries, and let all of your enemies be cut off.</td>\n",
              "      <td>hand</td>\n",
              "      <td>0.267857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3QREJ3J433XSBS8QMHAICCR0BQ1LKR</td>\n",
              "      <td>bible</td>\n",
              "      <td>Abimelech chased him, and he fled before him, and many fell wounded, even to the entrance of the gate.</td>\n",
              "      <td>entrance</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               id corpus  ...     token complexity\n",
              "0  3K8CQCU3KE19US5SN890DFPK3SANWR  bible  ...      hand   0.000000\n",
              "1  3Q2T3FD0ON86LCI41NJYV3PN0BW3MV  bible  ...      hand   0.197368\n",
              "2  3ULIZ0H1VA5C32JJMKOTQ8Z4GUS51B  bible  ...      hand   0.200000\n",
              "3  3BFF0DJK8XCEIOT30ZLBPPSRMZQTSD  bible  ...      hand   0.267857\n",
              "4  3QREJ3J433XSBS8QMHAICCR0BQ1LKR  bible  ...  entrance   0.000000\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN3ToWiZQlyk"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPdQRWvvaQyX"
      },
      "source": [
        "Try linear regression first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYqivG31Lubb"
      },
      "source": [
        "def create_weights_matrix(vocab, dimension=100):\n",
        "  \"\"\" create a matrix containing vectors for each word in Glove \"\"\"\n",
        "  matrix_len = len(vocab)\n",
        "  weights_matrix = np.zeros((matrix_len, dimension))\n",
        "\n",
        "  for i, word in enumerate(vocab):\n",
        "      try: \n",
        "          weights_matrix[i] = glove[word]\n",
        "      except KeyError:\n",
        "          weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, )) # initialize a random vector\n",
        "  #return torch.from_numpy(weights_matrix) # a tensor\n",
        "  return weights_matrix"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhOsDaEgacAF"
      },
      "source": [
        "# use the Glove 6B 100d\n",
        "cache_dir = \"/content/gdrive/My Drive/Colab Notebooks/data\"\n",
        "# glove = vocab.pretrained_aliases[\"glove.6B.100d\"](cache=cache_dir)\n",
        "glove = torchtext.vocab.GloVe(name='6B', dim=100, cache=cache_dir)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgCVuZWyZEXm",
        "outputId": "5dc777ed-5174-47e6-de22-a268da88e508"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0x_EbtYiab9v",
        "outputId": "45cbc5c3-7824-4108-b053-bf7ae2d44677"
      },
      "source": [
        "# get all the non-unique tokens for prediction\n",
        "tokens = df['token'].dropna().to_list()\n",
        "tokens = [token.lower() for token in tokens] # lowercase\n",
        "print(len(tokens))\n",
        "\n",
        "# check if all tokens are in Glove\n",
        "for token in tokens:\n",
        "  if token not in glove.stoi:\n",
        "    print(\"Token Not Found: \")\n",
        "    print(token)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7659\n",
            "Token Not Found: \n",
            "perverseness\n",
            "Token Not Found: \n",
            "perverseness\n",
            "Token Not Found: \n",
            "perverseness\n",
            "Token Not Found: \n",
            "housetops\n",
            "Token Not Found: \n",
            "slanderers\n",
            "Token Not Found: \n",
            "plowmen\n",
            "Token Not Found: \n",
            "dainties\n",
            "Token Not Found: \n",
            "dainties\n",
            "Token Not Found: \n",
            "dainties\n",
            "Token Not Found: \n",
            "dainties\n",
            "Token Not Found: \n",
            "dainties\n",
            "Token Not Found: \n",
            "dunghill\n",
            "Token Not Found: \n",
            "carotids\n",
            "Token Not Found: \n",
            "tace\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "aaZuP5QmeGgf",
        "outputId": "1caf2848-f422-4440-fd5e-489af73dab6a"
      },
      "source": [
        "# create a dataframe for linear regression\n",
        "train_df = pd.DataFrame(tokens, columns =['token'])\n",
        "\n",
        "# add back complexity\n",
        "train_df['complexity'] = df['complexity']\n",
        "\n",
        "# word length\n",
        "train_df['word_length'] = train_df['token'].map(lambda x: len(x))\n",
        "\n",
        "# punctuations\n",
        "punc = string.punctuation\n",
        "\n",
        "# stop words\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "# word frequency\n",
        "# tokenize the whole curpus\n",
        "temp = df['sentence'].to_list()\n",
        "texts = []\n",
        "for sent in temp:\n",
        "  sent = sent.lower()\n",
        "  sent = ''.join([c for c in sent if c not in punc])\n",
        "  words = [word for word in sent.split(' ') if (word.isalpha() and word not in stop_words)]\n",
        "  texts += words\n",
        "# count frequency\n",
        "count = Counter(texts)\n",
        "train_df['word_frequency'] = train_df['token'].map(lambda x: count[x])\n",
        "\n",
        "train_df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>word_length</th>\n",
              "      <th>word_frequency</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>river</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.263889</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      token  complexity  word_length  word_frequency\n",
              "0     river    0.000000            5              26\n",
              "1  brothers    0.000000            8              36\n",
              "2  brothers    0.050000            8              36\n",
              "3  brothers    0.150000            8              36\n",
              "4  brothers    0.263889            8              36"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "KU-Kz_ThLuYD",
        "outputId": "ec1b2a9f-ad91-4d8d-b401-a178c70c6040"
      },
      "source": [
        "# create the weight matrix\n",
        "weight_matrix = create_weights_matrix(tokens)\n",
        "print(weight_matrix.shape)\n",
        "\n",
        "# combine\n",
        "weight_matrix_df = pd.DataFrame(weight_matrix)\n",
        "\n",
        "train_df_combined = pd.concat([train_df, weight_matrix_df], axis=1)\n",
        "train_df_combined.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7659, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>word_length</th>\n",
              "      <th>word_frequency</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>river</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5</td>\n",
              "      <td>26</td>\n",
              "      <td>-0.33249</td>\n",
              "      <td>-0.56631</td>\n",
              "      <td>0.54255</td>\n",
              "      <td>-0.11869</td>\n",
              "      <td>0.531290</td>\n",
              "      <td>-0.49381</td>\n",
              "      <td>0.64114</td>\n",
              "      <td>0.85982</td>\n",
              "      <td>0.39633</td>\n",
              "      <td>-1.53950</td>\n",
              "      <td>-0.30613</td>\n",
              "      <td>0.97267</td>\n",
              "      <td>-0.31192</td>\n",
              "      <td>-0.10311</td>\n",
              "      <td>0.359510</td>\n",
              "      <td>-0.60023</td>\n",
              "      <td>0.909830</td>\n",
              "      <td>-0.959540</td>\n",
              "      <td>-0.55375</td>\n",
              "      <td>0.082818</td>\n",
              "      <td>0.26711</td>\n",
              "      <td>0.64645</td>\n",
              "      <td>-0.098556</td>\n",
              "      <td>0.539240</td>\n",
              "      <td>-0.21810</td>\n",
              "      <td>-0.13430</td>\n",
              "      <td>-1.80700</td>\n",
              "      <td>-0.14879</td>\n",
              "      <td>0.39006</td>\n",
              "      <td>-0.62883</td>\n",
              "      <td>-0.38825</td>\n",
              "      <td>0.31925</td>\n",
              "      <td>0.77853</td>\n",
              "      <td>-0.60273</td>\n",
              "      <td>0.063585</td>\n",
              "      <td>-0.75916</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.53185</td>\n",
              "      <td>0.72585</td>\n",
              "      <td>0.36811</td>\n",
              "      <td>0.19494</td>\n",
              "      <td>0.64276</td>\n",
              "      <td>0.81460</td>\n",
              "      <td>0.26748</td>\n",
              "      <td>-0.39275</td>\n",
              "      <td>0.425950</td>\n",
              "      <td>0.11699</td>\n",
              "      <td>0.21063</td>\n",
              "      <td>-0.061747</td>\n",
              "      <td>0.79298</td>\n",
              "      <td>-0.45978</td>\n",
              "      <td>0.85176</td>\n",
              "      <td>-0.36726</td>\n",
              "      <td>0.11816</td>\n",
              "      <td>0.504160</td>\n",
              "      <td>-0.065352</td>\n",
              "      <td>0.69672</td>\n",
              "      <td>0.37525</td>\n",
              "      <td>0.92586</td>\n",
              "      <td>-0.83036</td>\n",
              "      <td>-0.087948</td>\n",
              "      <td>-0.49715</td>\n",
              "      <td>0.21411</td>\n",
              "      <td>-0.82838</td>\n",
              "      <td>-0.85912</td>\n",
              "      <td>0.61576</td>\n",
              "      <td>1.18800</td>\n",
              "      <td>-0.30745</td>\n",
              "      <td>-1.20090</td>\n",
              "      <td>-1.70970</td>\n",
              "      <td>0.51400</td>\n",
              "      <td>-1.01590</td>\n",
              "      <td>0.55555</td>\n",
              "      <td>-1.03850</td>\n",
              "      <td>-0.69940</td>\n",
              "      <td>1.050600</td>\n",
              "      <td>0.24051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "      <td>0.85968</td>\n",
              "      <td>-0.39038</td>\n",
              "      <td>-0.62678</td>\n",
              "      <td>-0.55279</td>\n",
              "      <td>0.097012</td>\n",
              "      <td>0.00658</td>\n",
              "      <td>-0.65021</td>\n",
              "      <td>-0.58272</td>\n",
              "      <td>-1.27630</td>\n",
              "      <td>0.11251</td>\n",
              "      <td>0.78504</td>\n",
              "      <td>0.16027</td>\n",
              "      <td>0.38327</td>\n",
              "      <td>0.62672</td>\n",
              "      <td>-0.017462</td>\n",
              "      <td>-0.36443</td>\n",
              "      <td>0.062441</td>\n",
              "      <td>0.039266</td>\n",
              "      <td>-0.47318</td>\n",
              "      <td>0.547680</td>\n",
              "      <td>0.42916</td>\n",
              "      <td>-0.25516</td>\n",
              "      <td>0.100900</td>\n",
              "      <td>0.041618</td>\n",
              "      <td>-0.14579</td>\n",
              "      <td>0.15174</td>\n",
              "      <td>-0.54301</td>\n",
              "      <td>-0.29787</td>\n",
              "      <td>0.36268</td>\n",
              "      <td>0.89550</td>\n",
              "      <td>0.65319</td>\n",
              "      <td>0.40141</td>\n",
              "      <td>0.03668</td>\n",
              "      <td>-0.34313</td>\n",
              "      <td>-0.102040</td>\n",
              "      <td>-0.19383</td>\n",
              "      <td>...</td>\n",
              "      <td>0.65085</td>\n",
              "      <td>0.97366</td>\n",
              "      <td>0.36997</td>\n",
              "      <td>-0.58266</td>\n",
              "      <td>0.58389</td>\n",
              "      <td>-0.62574</td>\n",
              "      <td>-0.24252</td>\n",
              "      <td>1.37500</td>\n",
              "      <td>-0.042651</td>\n",
              "      <td>0.16398</td>\n",
              "      <td>-0.53462</td>\n",
              "      <td>0.552750</td>\n",
              "      <td>-0.58019</td>\n",
              "      <td>-0.78386</td>\n",
              "      <td>-0.18787</td>\n",
              "      <td>-0.20305</td>\n",
              "      <td>0.11506</td>\n",
              "      <td>-0.089296</td>\n",
              "      <td>-0.766080</td>\n",
              "      <td>0.04339</td>\n",
              "      <td>0.50251</td>\n",
              "      <td>0.73799</td>\n",
              "      <td>0.23388</td>\n",
              "      <td>0.200380</td>\n",
              "      <td>-0.93906</td>\n",
              "      <td>-0.33974</td>\n",
              "      <td>-0.56534</td>\n",
              "      <td>-0.95945</td>\n",
              "      <td>-0.14597</td>\n",
              "      <td>-0.35173</td>\n",
              "      <td>-0.40463</td>\n",
              "      <td>-0.32671</td>\n",
              "      <td>0.24982</td>\n",
              "      <td>-0.27804</td>\n",
              "      <td>-0.99877</td>\n",
              "      <td>-0.39367</td>\n",
              "      <td>-0.30087</td>\n",
              "      <td>-0.24623</td>\n",
              "      <td>0.006483</td>\n",
              "      <td>-0.21982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "      <td>0.85968</td>\n",
              "      <td>-0.39038</td>\n",
              "      <td>-0.62678</td>\n",
              "      <td>-0.55279</td>\n",
              "      <td>0.097012</td>\n",
              "      <td>0.00658</td>\n",
              "      <td>-0.65021</td>\n",
              "      <td>-0.58272</td>\n",
              "      <td>-1.27630</td>\n",
              "      <td>0.11251</td>\n",
              "      <td>0.78504</td>\n",
              "      <td>0.16027</td>\n",
              "      <td>0.38327</td>\n",
              "      <td>0.62672</td>\n",
              "      <td>-0.017462</td>\n",
              "      <td>-0.36443</td>\n",
              "      <td>0.062441</td>\n",
              "      <td>0.039266</td>\n",
              "      <td>-0.47318</td>\n",
              "      <td>0.547680</td>\n",
              "      <td>0.42916</td>\n",
              "      <td>-0.25516</td>\n",
              "      <td>0.100900</td>\n",
              "      <td>0.041618</td>\n",
              "      <td>-0.14579</td>\n",
              "      <td>0.15174</td>\n",
              "      <td>-0.54301</td>\n",
              "      <td>-0.29787</td>\n",
              "      <td>0.36268</td>\n",
              "      <td>0.89550</td>\n",
              "      <td>0.65319</td>\n",
              "      <td>0.40141</td>\n",
              "      <td>0.03668</td>\n",
              "      <td>-0.34313</td>\n",
              "      <td>-0.102040</td>\n",
              "      <td>-0.19383</td>\n",
              "      <td>...</td>\n",
              "      <td>0.65085</td>\n",
              "      <td>0.97366</td>\n",
              "      <td>0.36997</td>\n",
              "      <td>-0.58266</td>\n",
              "      <td>0.58389</td>\n",
              "      <td>-0.62574</td>\n",
              "      <td>-0.24252</td>\n",
              "      <td>1.37500</td>\n",
              "      <td>-0.042651</td>\n",
              "      <td>0.16398</td>\n",
              "      <td>-0.53462</td>\n",
              "      <td>0.552750</td>\n",
              "      <td>-0.58019</td>\n",
              "      <td>-0.78386</td>\n",
              "      <td>-0.18787</td>\n",
              "      <td>-0.20305</td>\n",
              "      <td>0.11506</td>\n",
              "      <td>-0.089296</td>\n",
              "      <td>-0.766080</td>\n",
              "      <td>0.04339</td>\n",
              "      <td>0.50251</td>\n",
              "      <td>0.73799</td>\n",
              "      <td>0.23388</td>\n",
              "      <td>0.200380</td>\n",
              "      <td>-0.93906</td>\n",
              "      <td>-0.33974</td>\n",
              "      <td>-0.56534</td>\n",
              "      <td>-0.95945</td>\n",
              "      <td>-0.14597</td>\n",
              "      <td>-0.35173</td>\n",
              "      <td>-0.40463</td>\n",
              "      <td>-0.32671</td>\n",
              "      <td>0.24982</td>\n",
              "      <td>-0.27804</td>\n",
              "      <td>-0.99877</td>\n",
              "      <td>-0.39367</td>\n",
              "      <td>-0.30087</td>\n",
              "      <td>-0.24623</td>\n",
              "      <td>0.006483</td>\n",
              "      <td>-0.21982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "      <td>0.85968</td>\n",
              "      <td>-0.39038</td>\n",
              "      <td>-0.62678</td>\n",
              "      <td>-0.55279</td>\n",
              "      <td>0.097012</td>\n",
              "      <td>0.00658</td>\n",
              "      <td>-0.65021</td>\n",
              "      <td>-0.58272</td>\n",
              "      <td>-1.27630</td>\n",
              "      <td>0.11251</td>\n",
              "      <td>0.78504</td>\n",
              "      <td>0.16027</td>\n",
              "      <td>0.38327</td>\n",
              "      <td>0.62672</td>\n",
              "      <td>-0.017462</td>\n",
              "      <td>-0.36443</td>\n",
              "      <td>0.062441</td>\n",
              "      <td>0.039266</td>\n",
              "      <td>-0.47318</td>\n",
              "      <td>0.547680</td>\n",
              "      <td>0.42916</td>\n",
              "      <td>-0.25516</td>\n",
              "      <td>0.100900</td>\n",
              "      <td>0.041618</td>\n",
              "      <td>-0.14579</td>\n",
              "      <td>0.15174</td>\n",
              "      <td>-0.54301</td>\n",
              "      <td>-0.29787</td>\n",
              "      <td>0.36268</td>\n",
              "      <td>0.89550</td>\n",
              "      <td>0.65319</td>\n",
              "      <td>0.40141</td>\n",
              "      <td>0.03668</td>\n",
              "      <td>-0.34313</td>\n",
              "      <td>-0.102040</td>\n",
              "      <td>-0.19383</td>\n",
              "      <td>...</td>\n",
              "      <td>0.65085</td>\n",
              "      <td>0.97366</td>\n",
              "      <td>0.36997</td>\n",
              "      <td>-0.58266</td>\n",
              "      <td>0.58389</td>\n",
              "      <td>-0.62574</td>\n",
              "      <td>-0.24252</td>\n",
              "      <td>1.37500</td>\n",
              "      <td>-0.042651</td>\n",
              "      <td>0.16398</td>\n",
              "      <td>-0.53462</td>\n",
              "      <td>0.552750</td>\n",
              "      <td>-0.58019</td>\n",
              "      <td>-0.78386</td>\n",
              "      <td>-0.18787</td>\n",
              "      <td>-0.20305</td>\n",
              "      <td>0.11506</td>\n",
              "      <td>-0.089296</td>\n",
              "      <td>-0.766080</td>\n",
              "      <td>0.04339</td>\n",
              "      <td>0.50251</td>\n",
              "      <td>0.73799</td>\n",
              "      <td>0.23388</td>\n",
              "      <td>0.200380</td>\n",
              "      <td>-0.93906</td>\n",
              "      <td>-0.33974</td>\n",
              "      <td>-0.56534</td>\n",
              "      <td>-0.95945</td>\n",
              "      <td>-0.14597</td>\n",
              "      <td>-0.35173</td>\n",
              "      <td>-0.40463</td>\n",
              "      <td>-0.32671</td>\n",
              "      <td>0.24982</td>\n",
              "      <td>-0.27804</td>\n",
              "      <td>-0.99877</td>\n",
              "      <td>-0.39367</td>\n",
              "      <td>-0.30087</td>\n",
              "      <td>-0.24623</td>\n",
              "      <td>0.006483</td>\n",
              "      <td>-0.21982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.263889</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "      <td>0.85968</td>\n",
              "      <td>-0.39038</td>\n",
              "      <td>-0.62678</td>\n",
              "      <td>-0.55279</td>\n",
              "      <td>0.097012</td>\n",
              "      <td>0.00658</td>\n",
              "      <td>-0.65021</td>\n",
              "      <td>-0.58272</td>\n",
              "      <td>-1.27630</td>\n",
              "      <td>0.11251</td>\n",
              "      <td>0.78504</td>\n",
              "      <td>0.16027</td>\n",
              "      <td>0.38327</td>\n",
              "      <td>0.62672</td>\n",
              "      <td>-0.017462</td>\n",
              "      <td>-0.36443</td>\n",
              "      <td>0.062441</td>\n",
              "      <td>0.039266</td>\n",
              "      <td>-0.47318</td>\n",
              "      <td>0.547680</td>\n",
              "      <td>0.42916</td>\n",
              "      <td>-0.25516</td>\n",
              "      <td>0.100900</td>\n",
              "      <td>0.041618</td>\n",
              "      <td>-0.14579</td>\n",
              "      <td>0.15174</td>\n",
              "      <td>-0.54301</td>\n",
              "      <td>-0.29787</td>\n",
              "      <td>0.36268</td>\n",
              "      <td>0.89550</td>\n",
              "      <td>0.65319</td>\n",
              "      <td>0.40141</td>\n",
              "      <td>0.03668</td>\n",
              "      <td>-0.34313</td>\n",
              "      <td>-0.102040</td>\n",
              "      <td>-0.19383</td>\n",
              "      <td>...</td>\n",
              "      <td>0.65085</td>\n",
              "      <td>0.97366</td>\n",
              "      <td>0.36997</td>\n",
              "      <td>-0.58266</td>\n",
              "      <td>0.58389</td>\n",
              "      <td>-0.62574</td>\n",
              "      <td>-0.24252</td>\n",
              "      <td>1.37500</td>\n",
              "      <td>-0.042651</td>\n",
              "      <td>0.16398</td>\n",
              "      <td>-0.53462</td>\n",
              "      <td>0.552750</td>\n",
              "      <td>-0.58019</td>\n",
              "      <td>-0.78386</td>\n",
              "      <td>-0.18787</td>\n",
              "      <td>-0.20305</td>\n",
              "      <td>0.11506</td>\n",
              "      <td>-0.089296</td>\n",
              "      <td>-0.766080</td>\n",
              "      <td>0.04339</td>\n",
              "      <td>0.50251</td>\n",
              "      <td>0.73799</td>\n",
              "      <td>0.23388</td>\n",
              "      <td>0.200380</td>\n",
              "      <td>-0.93906</td>\n",
              "      <td>-0.33974</td>\n",
              "      <td>-0.56534</td>\n",
              "      <td>-0.95945</td>\n",
              "      <td>-0.14597</td>\n",
              "      <td>-0.35173</td>\n",
              "      <td>-0.40463</td>\n",
              "      <td>-0.32671</td>\n",
              "      <td>0.24982</td>\n",
              "      <td>-0.27804</td>\n",
              "      <td>-0.99877</td>\n",
              "      <td>-0.39367</td>\n",
              "      <td>-0.30087</td>\n",
              "      <td>-0.24623</td>\n",
              "      <td>0.006483</td>\n",
              "      <td>-0.21982</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 104 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      token  complexity  word_length  ...       97        98       99\n",
              "0     river    0.000000            5  ... -0.69940  1.050600  0.24051\n",
              "1  brothers    0.000000            8  ... -0.24623  0.006483 -0.21982\n",
              "2  brothers    0.050000            8  ... -0.24623  0.006483 -0.21982\n",
              "3  brothers    0.150000            8  ... -0.24623  0.006483 -0.21982\n",
              "4  brothers    0.263889            8  ... -0.24623  0.006483 -0.21982\n",
              "\n",
              "[5 rows x 104 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCNNFO_uLuVz"
      },
      "source": [
        "# get data for training\n",
        "X_train = train_df_combined.drop(columns=['token', 'complexity'])\n",
        "Y_train = train_df_combined['complexity']"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzmE0l_emB6S"
      },
      "source": [
        "# train linear regression\n",
        "lr = LinearRegression().fit(X_train, Y_train)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luTw7T0HmB4B"
      },
      "source": [
        "# predict\n",
        "Y_pred = lr.predict(X_train)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHqtrYN0mB1q",
        "outputId": "aff43f43-cc34-4ebc-ded6-8b055cbceb70"
      },
      "source": [
        "# train loss (average absolute loss)\n",
        "num = len(Y_pred)\n",
        "losses = []\n",
        "for i in range(num):\n",
        "  loss = abs(Y_pred[i] - Y_train[i])\n",
        "  losses.append(loss)\n",
        "abl = sum(losses) / num\n",
        "print(\"average training absolute loss is \" + str(abl))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average training absolute loss is 0.07246931733686796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "q98DV01SovTY",
        "outputId": "762ae7e2-d66e-46ad-846d-eb6fa76c3c21"
      },
      "source": [
        "# on test\n",
        "test_tokens = test['token'].dropna().to_list()\n",
        "test_tokens = [token.lower() for token in test_tokens] # lowercase\n",
        "print(len(test_tokens))\n",
        "\n",
        "# create a dataframe for linear regression\n",
        "test_df = pd.DataFrame(test_tokens, columns =['token'])\n",
        "\n",
        "# add back complexity\n",
        "test_df['complexity'] = test['complexity']\n",
        "\n",
        "# word length\n",
        "test_df['word_length'] = test_df['token'].map(lambda x: len(x))\n",
        "\n",
        "# word frequency\n",
        "# tokenize the whole curpus\n",
        "temp = test['sentence'].to_list()\n",
        "texts = []\n",
        "for sent in temp:\n",
        "  sent = sent.lower()\n",
        "  sent = ''.join([c for c in sent if c not in punc])\n",
        "  words = [word for word in sent.split(' ') if (word.isalpha() and word not in stop_words)]\n",
        "  texts += words\n",
        "# count frequency\n",
        "count = Counter(texts)\n",
        "test_df['word_frequency'] = test_df['token'].map(lambda x: count[x])\n",
        "\n",
        "# create the weight matrix\n",
        "weight_matrix = create_weights_matrix(test_tokens)\n",
        "print(weight_matrix.shape)\n",
        "\n",
        "# combine\n",
        "weight_matrix_df = pd.DataFrame(weight_matrix)\n",
        "test_df_combined = pd.concat([test_df, weight_matrix_df], axis=1)\n",
        "test_df_combined.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "917\n",
            "(917, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>word_length</th>\n",
              "      <th>word_frequency</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hand</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>-0.24945</td>\n",
              "      <td>0.37033</td>\n",
              "      <td>-0.058334</td>\n",
              "      <td>-0.25367</td>\n",
              "      <td>0.18709</td>\n",
              "      <td>0.81760</td>\n",
              "      <td>-0.045494</td>\n",
              "      <td>0.072066</td>\n",
              "      <td>-0.059079</td>\n",
              "      <td>-0.053018</td>\n",
              "      <td>-0.15681</td>\n",
              "      <td>-0.18621</td>\n",
              "      <td>0.786770</td>\n",
              "      <td>0.56263</td>\n",
              "      <td>0.023693</td>\n",
              "      <td>0.241160</td>\n",
              "      <td>0.034775</td>\n",
              "      <td>0.117630</td>\n",
              "      <td>-0.15757</td>\n",
              "      <td>-0.39749</td>\n",
              "      <td>0.210680</td>\n",
              "      <td>-0.14618</td>\n",
              "      <td>0.014017</td>\n",
              "      <td>-0.22373</td>\n",
              "      <td>0.54225</td>\n",
              "      <td>0.47379</td>\n",
              "      <td>-0.62683</td>\n",
              "      <td>-0.38803</td>\n",
              "      <td>0.27510</td>\n",
              "      <td>-0.54687</td>\n",
              "      <td>0.49211</td>\n",
              "      <td>0.052715</td>\n",
              "      <td>-0.12911</td>\n",
              "      <td>0.2554</td>\n",
              "      <td>-0.005657</td>\n",
              "      <td>-0.194310</td>\n",
              "      <td>...</td>\n",
              "      <td>0.64518</td>\n",
              "      <td>0.76559</td>\n",
              "      <td>-0.22193</td>\n",
              "      <td>0.39305</td>\n",
              "      <td>0.13373</td>\n",
              "      <td>-0.17641</td>\n",
              "      <td>0.36222</td>\n",
              "      <td>0.47786</td>\n",
              "      <td>-0.43591</td>\n",
              "      <td>-0.13363</td>\n",
              "      <td>-0.13145</td>\n",
              "      <td>0.206730</td>\n",
              "      <td>0.37353</td>\n",
              "      <td>-0.70188</td>\n",
              "      <td>0.53225</td>\n",
              "      <td>0.103710</td>\n",
              "      <td>-0.70940</td>\n",
              "      <td>0.24331</td>\n",
              "      <td>-0.15523</td>\n",
              "      <td>0.20785</td>\n",
              "      <td>1.19970</td>\n",
              "      <td>-0.036297</td>\n",
              "      <td>-0.79044</td>\n",
              "      <td>-0.27794</td>\n",
              "      <td>-1.40760</td>\n",
              "      <td>-0.363180</td>\n",
              "      <td>0.40219</td>\n",
              "      <td>0.17401</td>\n",
              "      <td>-0.080981</td>\n",
              "      <td>-0.40688</td>\n",
              "      <td>-0.044007</td>\n",
              "      <td>-0.14964</td>\n",
              "      <td>0.39369</td>\n",
              "      <td>-0.014732</td>\n",
              "      <td>-0.41309</td>\n",
              "      <td>-0.061931</td>\n",
              "      <td>-0.088387</td>\n",
              "      <td>-0.230930</td>\n",
              "      <td>0.93931</td>\n",
              "      <td>0.091475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hand</td>\n",
              "      <td>0.197368</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>-0.24945</td>\n",
              "      <td>0.37033</td>\n",
              "      <td>-0.058334</td>\n",
              "      <td>-0.25367</td>\n",
              "      <td>0.18709</td>\n",
              "      <td>0.81760</td>\n",
              "      <td>-0.045494</td>\n",
              "      <td>0.072066</td>\n",
              "      <td>-0.059079</td>\n",
              "      <td>-0.053018</td>\n",
              "      <td>-0.15681</td>\n",
              "      <td>-0.18621</td>\n",
              "      <td>0.786770</td>\n",
              "      <td>0.56263</td>\n",
              "      <td>0.023693</td>\n",
              "      <td>0.241160</td>\n",
              "      <td>0.034775</td>\n",
              "      <td>0.117630</td>\n",
              "      <td>-0.15757</td>\n",
              "      <td>-0.39749</td>\n",
              "      <td>0.210680</td>\n",
              "      <td>-0.14618</td>\n",
              "      <td>0.014017</td>\n",
              "      <td>-0.22373</td>\n",
              "      <td>0.54225</td>\n",
              "      <td>0.47379</td>\n",
              "      <td>-0.62683</td>\n",
              "      <td>-0.38803</td>\n",
              "      <td>0.27510</td>\n",
              "      <td>-0.54687</td>\n",
              "      <td>0.49211</td>\n",
              "      <td>0.052715</td>\n",
              "      <td>-0.12911</td>\n",
              "      <td>0.2554</td>\n",
              "      <td>-0.005657</td>\n",
              "      <td>-0.194310</td>\n",
              "      <td>...</td>\n",
              "      <td>0.64518</td>\n",
              "      <td>0.76559</td>\n",
              "      <td>-0.22193</td>\n",
              "      <td>0.39305</td>\n",
              "      <td>0.13373</td>\n",
              "      <td>-0.17641</td>\n",
              "      <td>0.36222</td>\n",
              "      <td>0.47786</td>\n",
              "      <td>-0.43591</td>\n",
              "      <td>-0.13363</td>\n",
              "      <td>-0.13145</td>\n",
              "      <td>0.206730</td>\n",
              "      <td>0.37353</td>\n",
              "      <td>-0.70188</td>\n",
              "      <td>0.53225</td>\n",
              "      <td>0.103710</td>\n",
              "      <td>-0.70940</td>\n",
              "      <td>0.24331</td>\n",
              "      <td>-0.15523</td>\n",
              "      <td>0.20785</td>\n",
              "      <td>1.19970</td>\n",
              "      <td>-0.036297</td>\n",
              "      <td>-0.79044</td>\n",
              "      <td>-0.27794</td>\n",
              "      <td>-1.40760</td>\n",
              "      <td>-0.363180</td>\n",
              "      <td>0.40219</td>\n",
              "      <td>0.17401</td>\n",
              "      <td>-0.080981</td>\n",
              "      <td>-0.40688</td>\n",
              "      <td>-0.044007</td>\n",
              "      <td>-0.14964</td>\n",
              "      <td>0.39369</td>\n",
              "      <td>-0.014732</td>\n",
              "      <td>-0.41309</td>\n",
              "      <td>-0.061931</td>\n",
              "      <td>-0.088387</td>\n",
              "      <td>-0.230930</td>\n",
              "      <td>0.93931</td>\n",
              "      <td>0.091475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hand</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>-0.24945</td>\n",
              "      <td>0.37033</td>\n",
              "      <td>-0.058334</td>\n",
              "      <td>-0.25367</td>\n",
              "      <td>0.18709</td>\n",
              "      <td>0.81760</td>\n",
              "      <td>-0.045494</td>\n",
              "      <td>0.072066</td>\n",
              "      <td>-0.059079</td>\n",
              "      <td>-0.053018</td>\n",
              "      <td>-0.15681</td>\n",
              "      <td>-0.18621</td>\n",
              "      <td>0.786770</td>\n",
              "      <td>0.56263</td>\n",
              "      <td>0.023693</td>\n",
              "      <td>0.241160</td>\n",
              "      <td>0.034775</td>\n",
              "      <td>0.117630</td>\n",
              "      <td>-0.15757</td>\n",
              "      <td>-0.39749</td>\n",
              "      <td>0.210680</td>\n",
              "      <td>-0.14618</td>\n",
              "      <td>0.014017</td>\n",
              "      <td>-0.22373</td>\n",
              "      <td>0.54225</td>\n",
              "      <td>0.47379</td>\n",
              "      <td>-0.62683</td>\n",
              "      <td>-0.38803</td>\n",
              "      <td>0.27510</td>\n",
              "      <td>-0.54687</td>\n",
              "      <td>0.49211</td>\n",
              "      <td>0.052715</td>\n",
              "      <td>-0.12911</td>\n",
              "      <td>0.2554</td>\n",
              "      <td>-0.005657</td>\n",
              "      <td>-0.194310</td>\n",
              "      <td>...</td>\n",
              "      <td>0.64518</td>\n",
              "      <td>0.76559</td>\n",
              "      <td>-0.22193</td>\n",
              "      <td>0.39305</td>\n",
              "      <td>0.13373</td>\n",
              "      <td>-0.17641</td>\n",
              "      <td>0.36222</td>\n",
              "      <td>0.47786</td>\n",
              "      <td>-0.43591</td>\n",
              "      <td>-0.13363</td>\n",
              "      <td>-0.13145</td>\n",
              "      <td>0.206730</td>\n",
              "      <td>0.37353</td>\n",
              "      <td>-0.70188</td>\n",
              "      <td>0.53225</td>\n",
              "      <td>0.103710</td>\n",
              "      <td>-0.70940</td>\n",
              "      <td>0.24331</td>\n",
              "      <td>-0.15523</td>\n",
              "      <td>0.20785</td>\n",
              "      <td>1.19970</td>\n",
              "      <td>-0.036297</td>\n",
              "      <td>-0.79044</td>\n",
              "      <td>-0.27794</td>\n",
              "      <td>-1.40760</td>\n",
              "      <td>-0.363180</td>\n",
              "      <td>0.40219</td>\n",
              "      <td>0.17401</td>\n",
              "      <td>-0.080981</td>\n",
              "      <td>-0.40688</td>\n",
              "      <td>-0.044007</td>\n",
              "      <td>-0.14964</td>\n",
              "      <td>0.39369</td>\n",
              "      <td>-0.014732</td>\n",
              "      <td>-0.41309</td>\n",
              "      <td>-0.061931</td>\n",
              "      <td>-0.088387</td>\n",
              "      <td>-0.230930</td>\n",
              "      <td>0.93931</td>\n",
              "      <td>0.091475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hand</td>\n",
              "      <td>0.267857</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>-0.24945</td>\n",
              "      <td>0.37033</td>\n",
              "      <td>-0.058334</td>\n",
              "      <td>-0.25367</td>\n",
              "      <td>0.18709</td>\n",
              "      <td>0.81760</td>\n",
              "      <td>-0.045494</td>\n",
              "      <td>0.072066</td>\n",
              "      <td>-0.059079</td>\n",
              "      <td>-0.053018</td>\n",
              "      <td>-0.15681</td>\n",
              "      <td>-0.18621</td>\n",
              "      <td>0.786770</td>\n",
              "      <td>0.56263</td>\n",
              "      <td>0.023693</td>\n",
              "      <td>0.241160</td>\n",
              "      <td>0.034775</td>\n",
              "      <td>0.117630</td>\n",
              "      <td>-0.15757</td>\n",
              "      <td>-0.39749</td>\n",
              "      <td>0.210680</td>\n",
              "      <td>-0.14618</td>\n",
              "      <td>0.014017</td>\n",
              "      <td>-0.22373</td>\n",
              "      <td>0.54225</td>\n",
              "      <td>0.47379</td>\n",
              "      <td>-0.62683</td>\n",
              "      <td>-0.38803</td>\n",
              "      <td>0.27510</td>\n",
              "      <td>-0.54687</td>\n",
              "      <td>0.49211</td>\n",
              "      <td>0.052715</td>\n",
              "      <td>-0.12911</td>\n",
              "      <td>0.2554</td>\n",
              "      <td>-0.005657</td>\n",
              "      <td>-0.194310</td>\n",
              "      <td>...</td>\n",
              "      <td>0.64518</td>\n",
              "      <td>0.76559</td>\n",
              "      <td>-0.22193</td>\n",
              "      <td>0.39305</td>\n",
              "      <td>0.13373</td>\n",
              "      <td>-0.17641</td>\n",
              "      <td>0.36222</td>\n",
              "      <td>0.47786</td>\n",
              "      <td>-0.43591</td>\n",
              "      <td>-0.13363</td>\n",
              "      <td>-0.13145</td>\n",
              "      <td>0.206730</td>\n",
              "      <td>0.37353</td>\n",
              "      <td>-0.70188</td>\n",
              "      <td>0.53225</td>\n",
              "      <td>0.103710</td>\n",
              "      <td>-0.70940</td>\n",
              "      <td>0.24331</td>\n",
              "      <td>-0.15523</td>\n",
              "      <td>0.20785</td>\n",
              "      <td>1.19970</td>\n",
              "      <td>-0.036297</td>\n",
              "      <td>-0.79044</td>\n",
              "      <td>-0.27794</td>\n",
              "      <td>-1.40760</td>\n",
              "      <td>-0.363180</td>\n",
              "      <td>0.40219</td>\n",
              "      <td>0.17401</td>\n",
              "      <td>-0.080981</td>\n",
              "      <td>-0.40688</td>\n",
              "      <td>-0.044007</td>\n",
              "      <td>-0.14964</td>\n",
              "      <td>0.39369</td>\n",
              "      <td>-0.014732</td>\n",
              "      <td>-0.41309</td>\n",
              "      <td>-0.061931</td>\n",
              "      <td>-0.088387</td>\n",
              "      <td>-0.230930</td>\n",
              "      <td>0.93931</td>\n",
              "      <td>0.091475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>entrance</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>0.25776</td>\n",
              "      <td>0.10680</td>\n",
              "      <td>-0.162650</td>\n",
              "      <td>0.42335</td>\n",
              "      <td>0.19078</td>\n",
              "      <td>0.46283</td>\n",
              "      <td>-0.959150</td>\n",
              "      <td>0.931740</td>\n",
              "      <td>0.471610</td>\n",
              "      <td>0.390770</td>\n",
              "      <td>0.54734</td>\n",
              "      <td>0.41967</td>\n",
              "      <td>0.086822</td>\n",
              "      <td>0.53954</td>\n",
              "      <td>0.354970</td>\n",
              "      <td>-0.028346</td>\n",
              "      <td>0.427080</td>\n",
              "      <td>0.036569</td>\n",
              "      <td>-0.49700</td>\n",
              "      <td>-0.49543</td>\n",
              "      <td>-0.031232</td>\n",
              "      <td>-0.30298</td>\n",
              "      <td>-0.417180</td>\n",
              "      <td>-0.78459</td>\n",
              "      <td>0.70473</td>\n",
              "      <td>-0.59741</td>\n",
              "      <td>-0.33173</td>\n",
              "      <td>-0.38813</td>\n",
              "      <td>0.17189</td>\n",
              "      <td>-0.78565</td>\n",
              "      <td>-0.17219</td>\n",
              "      <td>-0.140190</td>\n",
              "      <td>0.61492</td>\n",
              "      <td>0.5713</td>\n",
              "      <td>0.751090</td>\n",
              "      <td>-0.015942</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.60393</td>\n",
              "      <td>0.47454</td>\n",
              "      <td>0.80912</td>\n",
              "      <td>0.81709</td>\n",
              "      <td>-0.12876</td>\n",
              "      <td>-0.39310</td>\n",
              "      <td>0.17656</td>\n",
              "      <td>-0.29797</td>\n",
              "      <td>-0.32614</td>\n",
              "      <td>-0.26522</td>\n",
              "      <td>-0.37006</td>\n",
              "      <td>-0.016956</td>\n",
              "      <td>0.92268</td>\n",
              "      <td>-0.71606</td>\n",
              "      <td>-0.38524</td>\n",
              "      <td>-0.085737</td>\n",
              "      <td>0.68111</td>\n",
              "      <td>0.32080</td>\n",
              "      <td>0.45870</td>\n",
              "      <td>-0.82737</td>\n",
              "      <td>0.22932</td>\n",
              "      <td>0.314500</td>\n",
              "      <td>-0.21221</td>\n",
              "      <td>-0.65293</td>\n",
              "      <td>-0.31427</td>\n",
              "      <td>-0.037493</td>\n",
              "      <td>0.16126</td>\n",
              "      <td>-0.46719</td>\n",
              "      <td>0.630660</td>\n",
              "      <td>0.26426</td>\n",
              "      <td>0.527780</td>\n",
              "      <td>-0.34505</td>\n",
              "      <td>0.06620</td>\n",
              "      <td>0.722400</td>\n",
              "      <td>-0.11057</td>\n",
              "      <td>-0.005771</td>\n",
              "      <td>-0.059336</td>\n",
              "      <td>0.013272</td>\n",
              "      <td>0.97305</td>\n",
              "      <td>0.454050</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 104 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      token  complexity  word_length  ...        97       98        99\n",
              "0      hand    0.000000            4  ... -0.230930  0.93931  0.091475\n",
              "1      hand    0.197368            4  ... -0.230930  0.93931  0.091475\n",
              "2      hand    0.200000            4  ... -0.230930  0.93931  0.091475\n",
              "3      hand    0.267857            4  ... -0.230930  0.93931  0.091475\n",
              "4  entrance    0.000000            8  ...  0.013272  0.97305  0.454050\n",
              "\n",
              "[5 rows x 104 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEy_z413ovRJ",
        "outputId": "0a47ca43-d8ea-45ea-ff21-9db99d5daae6"
      },
      "source": [
        "# get data for test\n",
        "X_test = test_df_combined.drop(columns=['token', 'complexity'])\n",
        "Y_test = test_df_combined['complexity']\n",
        "\n",
        "# predict\n",
        "Y_pred = lr.predict(X_test)\n",
        "\n",
        "# test loss (average absolute loss)\n",
        "num = len(Y_pred)\n",
        "losses = []\n",
        "for i in range(num):\n",
        "  loss = abs(Y_pred[i] - Y_test[i])\n",
        "  losses.append(loss)\n",
        "abl = sum(losses) / num\n",
        "print(\"average test absolute loss is \" + str(abl))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average test absolute loss is 0.07283375821746224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6xzRuFTovN5"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un9WFQkFUQcx"
      },
      "source": [
        "DATA PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "hDibiYSCLuf1",
        "outputId": "66b52f4e-1c3a-400a-9713-c0e0b9e8d23e"
      },
      "source": [
        "# tokenize sentences\n",
        "\n",
        "def tokenize(sent, token, punc, stop_words):\n",
        "  \"\"\" lowercase, padded, remove stopwords and punctuations \"\"\"\n",
        "  # lowercase\n",
        "  sent = sent.lower()\n",
        "  # remove punctuation and stopwords\n",
        "  sent = ''.join([c for c in sent if c not in punc]) \n",
        "  tokens = [word for word in sent.split(' ') if (word.isalpha() and word not in stop_words)]\n",
        "  # pad\n",
        "  tokens.insert(0, '<s>')\n",
        "  tokens.append('</s>')\n",
        "  # pad the token with special symbols\n",
        "  for i in range(len(tokens)):\n",
        "    if tokens[i] == token:\n",
        "      tokens.insert(i, '_START')\n",
        "      tokens.insert(i+2, '_END')\n",
        "      break\n",
        "\n",
        "  return tokens\n",
        "\n",
        "def get_complexity_level(n):\n",
        "  \"\"\" map complexity to corresponding level \"\"\"\n",
        "  # 1: 0, 2: 0:25, 3: 0:5, 4: 0:75, 5: 1\n",
        "  # I choose to use mid point to map decimal back to integer\n",
        "  if n <= 0.125:\n",
        "    return 1\n",
        "  elif n <= 0.375:\n",
        "    return 2\n",
        "  elif n <= 0.625:\n",
        "    return 3\n",
        "  elif n <= 0.875:\n",
        "    return 4 \n",
        "  return 5\n",
        "\n",
        "def preprocess(df):\n",
        "  data = df[['sentence', 'token', 'complexity']]\n",
        "  data['tokenized_sentence'] = data.apply(lambda row: tokenize(row['sentence'], row['token'], punc, stop_words), axis=1)\n",
        "  data['complexity'] = data['complexity'].map(lambda x: get_complexity_level(x))\n",
        "  data = data.drop(columns=['sentence'])\n",
        "  return data\n",
        "\n",
        "train_data = preprocess(df)\n",
        "test_data = preprocess(test)\n",
        "\n",
        "train_data.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>tokenized_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>river</td>\n",
              "      <td>1</td>\n",
              "      <td>[&lt;s&gt;, behold, came, _START, river, _END, seven, cattle, sleek, fat, fed, marsh, grass, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>brothers</td>\n",
              "      <td>1</td>\n",
              "      <td>[&lt;s&gt;, fellow, bondservant, _START, brothers, _END, prophets, keep, words, book, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>brothers</td>\n",
              "      <td>1</td>\n",
              "      <td>[&lt;s&gt;, man, lord, land, said, us, know, honest, men, leave, one, _START, brothers, _END, take, grain, famine, houses, go, way, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>brothers</td>\n",
              "      <td>2</td>\n",
              "      <td>[&lt;s&gt;, shimei, sixteen, sons, six, daughters, _START, brothers, _END, didnt, many, children, neither, family, multiply, like, children, judah, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>brothers</td>\n",
              "      <td>2</td>\n",
              "      <td>[&lt;s&gt;, put, _START, brothers, _END, far, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      token  ...                                                                                                                                   tokenized_sentence\n",
              "0     river  ...                                                         [<s>, behold, came, _START, river, _END, seven, cattle, sleek, fat, fed, marsh, grass, </s>]\n",
              "1  brothers  ...                                                                [<s>, fellow, bondservant, _START, brothers, _END, prophets, keep, words, book, </s>]\n",
              "2  brothers  ...                  [<s>, man, lord, land, said, us, know, honest, men, leave, one, _START, brothers, _END, take, grain, famine, houses, go, way, </s>]\n",
              "3  brothers  ...  [<s>, shimei, sixteen, sons, six, daughters, _START, brothers, _END, didnt, many, children, neither, family, multiply, like, children, judah, </s>]\n",
              "4  brothers  ...                                                                                                        [<s>, put, _START, brothers, _END, far, </s>]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7zco41RQrgD"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Jpf2sEdQraL",
        "outputId": "0fdb7425-c45f-485b-bb88-43b1ae03b444"
      },
      "source": [
        "# take a look\n",
        "temp1 = train_data[train_data['complexity'] == 1]\n",
        "temp2 = train_data[train_data['complexity'] == 2]\n",
        "temp3 = train_data[train_data['complexity'] == 3]\n",
        "temp4 = train_data[train_data['complexity'] == 4]\n",
        "temp5 = train_data[train_data['complexity'] == 5]\n",
        "\n",
        "print(\"1: \")\n",
        "print(temp1.shape)\n",
        "print(\"2: \")\n",
        "print(temp2.shape)\n",
        "print(\"3: \")\n",
        "print(temp3.shape)\n",
        "print(\"4: \")\n",
        "print(temp4.shape)\n",
        "print(\"5: \")\n",
        "print(temp5.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1: \n",
            "(430, 3)\n",
            "2: \n",
            "(5419, 3)\n",
            "3: \n",
            "(1646, 3)\n",
            "4: \n",
            "(167, 3)\n",
            "5: \n",
            "(0, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSGGceDJQo-0"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "l4fnXnBekvW3",
        "outputId": "5ce2e273-0cbd-48dd-c25d-56605401bbdc"
      },
      "source": [
        "# convert words to index for training and testing purpose\n",
        "sentences = train_data['tokenized_sentence'].to_list()\n",
        "temp = []\n",
        "for sent in sentences:\n",
        "  temp += sent\n",
        "temp = set(temp)\n",
        "# for words that are unknown\n",
        "temp.add('_UNKNOWN') \n",
        "temp.add('_PADDING')\n",
        "print(len(temp))\n",
        "\n",
        "# need to pad sentences to the same length\n",
        "lengths = [len(sent) for sent in sentences]\n",
        "pad_length = max(lengths)\n",
        "print(pad_length)\n",
        "\n",
        "# construct dictionaries\n",
        "word2index = {}\n",
        "index2word = {}\n",
        "for i, word in enumerate(temp):\n",
        "  word2index[word] = i\n",
        "  index2word[i] = word\n",
        "\n",
        "def word_to_index(sentence):\n",
        "  # sentence: a list of strings\n",
        "  r = []\n",
        "  for word in sentence:\n",
        "    if word in word2index:\n",
        "      r.append(word2index[word])\n",
        "    else:\n",
        "      r.append(word2index['_UNKNOWN'])\n",
        "  diff = pad_length - len(sentence)\n",
        "  pad_index = word2index['_PADDING']\n",
        "  for i in range(diff):\n",
        "    r.append(pad_index)\n",
        "  return r\n",
        "\n",
        "train_data['number_sentence'] = train_data['tokenized_sentence'].map(lambda sent: word_to_index(sent))\n",
        "test_data['number_sentence'] = test_data['tokenized_sentence'].map(lambda sent: word_to_index(sent))\n",
        "\n",
        "train_data = train_data.drop(columns=['tokenized_sentence'])\n",
        "test_data = test_data.drop(columns=['tokenized_sentence'])\n",
        "\n",
        "train_data.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14826\n",
            "118\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>number_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>river</td>\n",
              "      <td>1</td>\n",
              "      <td>[8925, 10407, 5450, 617, 7277, 12641, 2541, 12350, 11532, 2267, 12118, 12556, 4773, 6341, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>brothers</td>\n",
              "      <td>1</td>\n",
              "      <td>[8925, 8656, 3202, 617, 8675, 12641, 9380, 11263, 1568, 10702, 6341, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>brothers</td>\n",
              "      <td>1</td>\n",
              "      <td>[8925, 8534, 4547, 2980, 11095, 13452, 6515, 9974, 6985, 1711, 5919, 617, 8675, 12641, 7843, 11453, 8987, 11462, 10209, 9653, 6341, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>brothers</td>\n",
              "      <td>2</td>\n",
              "      <td>[8925, 9443, 10380, 448, 530, 1363, 617, 8675, 12641, 9484, 10288, 9649, 3803, 13392, 14758, 814, 9649, 11450, 6341, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>brothers</td>\n",
              "      <td>2</td>\n",
              "      <td>[8925, 6249, 617, 8675, 12641, 892, 6341, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, ...]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      token  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            number_sentence\n",
              "0     river  ...   [8925, 10407, 5450, 617, 7277, 12641, 2541, 12350, 11532, 2267, 12118, 12556, 4773, 6341, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, ...]\n",
              "1  brothers  ...   [8925, 8656, 3202, 617, 8675, 12641, 9380, 11263, 1568, 10702, 6341, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, ...]\n",
              "2  brothers  ...          [8925, 8534, 4547, 2980, 11095, 13452, 6515, 9974, 6985, 1711, 5919, 617, 8675, 12641, 7843, 11453, 8987, 11462, 10209, 9653, 6341, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, ...]\n",
              "3  brothers  ...           [8925, 9443, 10380, 448, 530, 1363, 617, 8675, 12641, 9484, 10288, 9649, 3803, 13392, 14758, 814, 9649, 11450, 6341, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, ...]\n",
              "4  brothers  ...  [8925, 6249, 617, 8675, 12641, 892, 6341, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, 11536, ...]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNw9bjg2Luda",
        "outputId": "7d991854-cba3-40b3-ca00-ceb6ef979a05"
      },
      "source": [
        "# do a simple check\n",
        "print(df.shape)\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)\n",
        "print(len(word2index.keys()))\n",
        "print(len(index2word.keys()))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7662, 5)\n",
            "(7662, 3)\n",
            "(917, 3)\n",
            "14826\n",
            "14826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL3ASkWeeGZo",
        "outputId": "1129e534-197a-48cf-98e9-923ff5a2ea7b"
      },
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOTh_SM0eGXC"
      },
      "source": [
        "train_sentences = np.array(train_data['number_sentence'].to_list())\n",
        "train_labels = np.array(train_data['complexity'].to_list())\n",
        "test_sentences = np.array(test_data['number_sentence'].to_list())\n",
        "test_labels = np.array(test_data['complexity'].to_list())\n",
        "\n",
        "training = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n",
        "testing = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(training, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "test_loader = DataLoader(testing, shuffle=True, batch_size=batch_size, drop_last=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmDPzyUOeGUo"
      },
      "source": [
        "# the LSTM class\n",
        "class SentimentNet(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        super(SentimentNet, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        #self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)       \n",
        "        out = self.fc(lstm_out[:, -1, :])      \n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        return hidden"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFAZtgekeGSh",
        "outputId": "b6dfe09d-e95d-4887-89be-c53f32f09eef"
      },
      "source": [
        "# some parameters\n",
        "vocab_size = len(word2index) + 1\n",
        "output_size = 5\n",
        "embedding_dim = 400\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "\n",
        "model = SentimentNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "model.to(device)\n",
        "print(model)\n",
        "\n",
        "lr=0.01\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "epochs = 1000\n",
        "counter = 0\n",
        "print_every = 256\n",
        "clip = 5"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentNet(\n",
            "  (embedding): Embedding(14827, 400)\n",
            "  (lstm): LSTM(400, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eggcDcSeGQJ",
        "outputId": "7806afbe-b6ab-44d8-9b87-a4dcf6abcf95"
      },
      "source": [
        "# training\n",
        "model.train()\n",
        "for i in range(epochs):\n",
        "  h = model.init_hidden(batch_size)\n",
        "    \n",
        "  for inputs, labels in train_loader:\n",
        "    counter += 1\n",
        "    h = tuple([e.data for e in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    model.zero_grad()\n",
        "    output, h = model(inputs, h)\n",
        "    # cross entropy for multiple classes\n",
        "    # output is of shape 64 * 5 while labels is of shape 64\n",
        "    loss = criterion(output, labels) \n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    if counter%print_every == 0:\n",
        "      print(\"Epoch: {}/{}...\".format(i+1, epochs), \"Step: {}...\".format(counter), \"Loss: {:.6f}...\".format(loss.item()))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 5/1000... Step: 256... Loss: 0.894739...\n",
            "Epoch: 9/1000... Step: 512... Loss: 0.987313...\n",
            "Epoch: 14/1000... Step: 768... Loss: 0.988397...\n",
            "Epoch: 18/1000... Step: 1024... Loss: 0.853481...\n",
            "Epoch: 22/1000... Step: 1280... Loss: 0.864996...\n",
            "Epoch: 27/1000... Step: 1536... Loss: 0.907211...\n",
            "Epoch: 31/1000... Step: 1792... Loss: 0.784693...\n",
            "Epoch: 35/1000... Step: 2048... Loss: 0.837231...\n",
            "Epoch: 40/1000... Step: 2304... Loss: 0.787397...\n",
            "Epoch: 44/1000... Step: 2560... Loss: 0.845988...\n",
            "Epoch: 48/1000... Step: 2816... Loss: 0.798551...\n",
            "Epoch: 53/1000... Step: 3072... Loss: 0.893498...\n",
            "Epoch: 57/1000... Step: 3328... Loss: 0.906740...\n",
            "Epoch: 61/1000... Step: 3584... Loss: 1.036262...\n",
            "Epoch: 66/1000... Step: 3840... Loss: 0.853069...\n",
            "Epoch: 70/1000... Step: 4096... Loss: 0.877835...\n",
            "Epoch: 74/1000... Step: 4352... Loss: 0.787577...\n",
            "Epoch: 79/1000... Step: 4608... Loss: 0.882855...\n",
            "Epoch: 83/1000... Step: 4864... Loss: 0.811329...\n",
            "Epoch: 87/1000... Step: 5120... Loss: 0.891699...\n",
            "Epoch: 92/1000... Step: 5376... Loss: 0.830554...\n",
            "Epoch: 96/1000... Step: 5632... Loss: 0.879348...\n",
            "Epoch: 100/1000... Step: 5888... Loss: 0.916367...\n",
            "Epoch: 105/1000... Step: 6144... Loss: 0.790872...\n",
            "Epoch: 109/1000... Step: 6400... Loss: 0.857031...\n",
            "Epoch: 113/1000... Step: 6656... Loss: 1.108533...\n",
            "Epoch: 118/1000... Step: 6912... Loss: 0.796389...\n",
            "Epoch: 122/1000... Step: 7168... Loss: 0.854705...\n",
            "Epoch: 126/1000... Step: 7424... Loss: 0.865223...\n",
            "Epoch: 131/1000... Step: 7680... Loss: 0.902766...\n",
            "Epoch: 135/1000... Step: 7936... Loss: 0.855804...\n",
            "Epoch: 139/1000... Step: 8192... Loss: 0.891191...\n",
            "Epoch: 144/1000... Step: 8448... Loss: 0.745078...\n",
            "Epoch: 148/1000... Step: 8704... Loss: 0.792934...\n",
            "Epoch: 152/1000... Step: 8960... Loss: 0.939034...\n",
            "Epoch: 157/1000... Step: 9216... Loss: 0.704836...\n",
            "Epoch: 161/1000... Step: 9472... Loss: 0.794384...\n",
            "Epoch: 165/1000... Step: 9728... Loss: 0.953078...\n",
            "Epoch: 170/1000... Step: 9984... Loss: 0.848106...\n",
            "Epoch: 174/1000... Step: 10240... Loss: 0.891127...\n",
            "Epoch: 178/1000... Step: 10496... Loss: 0.760918...\n",
            "Epoch: 183/1000... Step: 10752... Loss: 0.871157...\n",
            "Epoch: 187/1000... Step: 11008... Loss: 0.857406...\n",
            "Epoch: 191/1000... Step: 11264... Loss: 0.974627...\n",
            "Epoch: 196/1000... Step: 11520... Loss: 0.746390...\n",
            "Epoch: 200/1000... Step: 11776... Loss: 0.832328...\n",
            "Epoch: 204/1000... Step: 12032... Loss: 0.709930...\n",
            "Epoch: 209/1000... Step: 12288... Loss: 0.761047...\n",
            "Epoch: 213/1000... Step: 12544... Loss: 1.042724...\n",
            "Epoch: 217/1000... Step: 12800... Loss: 0.958208...\n",
            "Epoch: 222/1000... Step: 13056... Loss: 0.796610...\n",
            "Epoch: 226/1000... Step: 13312... Loss: 0.889192...\n",
            "Epoch: 230/1000... Step: 13568... Loss: 0.744524...\n",
            "Epoch: 235/1000... Step: 13824... Loss: 0.845832...\n",
            "Epoch: 239/1000... Step: 14080... Loss: 0.694932...\n",
            "Epoch: 243/1000... Step: 14336... Loss: 0.878949...\n",
            "Epoch: 248/1000... Step: 14592... Loss: 0.740018...\n",
            "Epoch: 252/1000... Step: 14848... Loss: 0.896055...\n",
            "Epoch: 256/1000... Step: 15104... Loss: 0.689047...\n",
            "Epoch: 261/1000... Step: 15360... Loss: 0.947318...\n",
            "Epoch: 265/1000... Step: 15616... Loss: 0.896132...\n",
            "Epoch: 270/1000... Step: 15872... Loss: 0.957040...\n",
            "Epoch: 274/1000... Step: 16128... Loss: 0.849266...\n",
            "Epoch: 278/1000... Step: 16384... Loss: 0.755560...\n",
            "Epoch: 283/1000... Step: 16640... Loss: 0.857410...\n",
            "Epoch: 287/1000... Step: 16896... Loss: 0.747256...\n",
            "Epoch: 291/1000... Step: 17152... Loss: 0.933856...\n",
            "Epoch: 296/1000... Step: 17408... Loss: 0.868596...\n",
            "Epoch: 300/1000... Step: 17664... Loss: 0.802221...\n",
            "Epoch: 304/1000... Step: 17920... Loss: 0.768074...\n",
            "Epoch: 309/1000... Step: 18176... Loss: 0.882811...\n",
            "Epoch: 313/1000... Step: 18432... Loss: 0.948029...\n",
            "Epoch: 317/1000... Step: 18688... Loss: 0.948683...\n",
            "Epoch: 322/1000... Step: 18944... Loss: 1.010132...\n",
            "Epoch: 326/1000... Step: 19200... Loss: 0.805889...\n",
            "Epoch: 330/1000... Step: 19456... Loss: 0.840003...\n",
            "Epoch: 335/1000... Step: 19712... Loss: 0.790412...\n",
            "Epoch: 339/1000... Step: 19968... Loss: 0.832769...\n",
            "Epoch: 343/1000... Step: 20224... Loss: 0.799061...\n",
            "Epoch: 348/1000... Step: 20480... Loss: 0.879671...\n",
            "Epoch: 352/1000... Step: 20736... Loss: 0.732520...\n",
            "Epoch: 356/1000... Step: 20992... Loss: 0.860589...\n",
            "Epoch: 361/1000... Step: 21248... Loss: 0.760123...\n",
            "Epoch: 365/1000... Step: 21504... Loss: 0.695901...\n",
            "Epoch: 369/1000... Step: 21760... Loss: 0.795334...\n",
            "Epoch: 374/1000... Step: 22016... Loss: 0.754228...\n",
            "Epoch: 378/1000... Step: 22272... Loss: 0.665120...\n",
            "Epoch: 382/1000... Step: 22528... Loss: 1.103049...\n",
            "Epoch: 387/1000... Step: 22784... Loss: 0.818822...\n",
            "Epoch: 391/1000... Step: 23040... Loss: 0.850103...\n",
            "Epoch: 395/1000... Step: 23296... Loss: 0.755072...\n",
            "Epoch: 400/1000... Step: 23552... Loss: 0.790052...\n",
            "Epoch: 404/1000... Step: 23808... Loss: 0.798761...\n",
            "Epoch: 408/1000... Step: 24064... Loss: 0.973487...\n",
            "Epoch: 413/1000... Step: 24320... Loss: 0.999967...\n",
            "Epoch: 417/1000... Step: 24576... Loss: 0.769440...\n",
            "Epoch: 421/1000... Step: 24832... Loss: 0.948304...\n",
            "Epoch: 426/1000... Step: 25088... Loss: 0.807533...\n",
            "Epoch: 430/1000... Step: 25344... Loss: 0.874347...\n",
            "Epoch: 434/1000... Step: 25600... Loss: 0.829718...\n",
            "Epoch: 439/1000... Step: 25856... Loss: 0.828359...\n",
            "Epoch: 443/1000... Step: 26112... Loss: 0.829767...\n",
            "Epoch: 447/1000... Step: 26368... Loss: 0.997718...\n",
            "Epoch: 452/1000... Step: 26624... Loss: 0.738263...\n",
            "Epoch: 456/1000... Step: 26880... Loss: 0.919249...\n",
            "Epoch: 460/1000... Step: 27136... Loss: 0.782789...\n",
            "Epoch: 465/1000... Step: 27392... Loss: 0.782468...\n",
            "Epoch: 469/1000... Step: 27648... Loss: 0.713686...\n",
            "Epoch: 473/1000... Step: 27904... Loss: 0.950959...\n",
            "Epoch: 478/1000... Step: 28160... Loss: 0.958350...\n",
            "Epoch: 482/1000... Step: 28416... Loss: 1.054946...\n",
            "Epoch: 486/1000... Step: 28672... Loss: 0.884017...\n",
            "Epoch: 491/1000... Step: 28928... Loss: 0.955750...\n",
            "Epoch: 495/1000... Step: 29184... Loss: 0.822040...\n",
            "Epoch: 499/1000... Step: 29440... Loss: 0.716755...\n",
            "Epoch: 504/1000... Step: 29696... Loss: 0.961799...\n",
            "Epoch: 508/1000... Step: 29952... Loss: 0.908558...\n",
            "Epoch: 512/1000... Step: 30208... Loss: 0.886201...\n",
            "Epoch: 517/1000... Step: 30464... Loss: 0.762056...\n",
            "Epoch: 521/1000... Step: 30720... Loss: 0.739613...\n",
            "Epoch: 526/1000... Step: 30976... Loss: 0.840047...\n",
            "Epoch: 530/1000... Step: 31232... Loss: 0.751571...\n",
            "Epoch: 534/1000... Step: 31488... Loss: 0.932233...\n",
            "Epoch: 539/1000... Step: 31744... Loss: 0.815450...\n",
            "Epoch: 543/1000... Step: 32000... Loss: 0.703329...\n",
            "Epoch: 547/1000... Step: 32256... Loss: 0.853099...\n",
            "Epoch: 552/1000... Step: 32512... Loss: 0.878265...\n",
            "Epoch: 556/1000... Step: 32768... Loss: 0.898898...\n",
            "Epoch: 560/1000... Step: 33024... Loss: 0.835810...\n",
            "Epoch: 565/1000... Step: 33280... Loss: 1.038830...\n",
            "Epoch: 569/1000... Step: 33536... Loss: 0.814430...\n",
            "Epoch: 573/1000... Step: 33792... Loss: 0.936612...\n",
            "Epoch: 578/1000... Step: 34048... Loss: 0.857170...\n",
            "Epoch: 582/1000... Step: 34304... Loss: 0.944791...\n",
            "Epoch: 586/1000... Step: 34560... Loss: 0.907127...\n",
            "Epoch: 591/1000... Step: 34816... Loss: 0.679403...\n",
            "Epoch: 595/1000... Step: 35072... Loss: 0.929186...\n",
            "Epoch: 599/1000... Step: 35328... Loss: 0.863718...\n",
            "Epoch: 604/1000... Step: 35584... Loss: 0.834388...\n",
            "Epoch: 608/1000... Step: 35840... Loss: 0.624717...\n",
            "Epoch: 612/1000... Step: 36096... Loss: 0.789009...\n",
            "Epoch: 617/1000... Step: 36352... Loss: 0.816361...\n",
            "Epoch: 621/1000... Step: 36608... Loss: 0.634379...\n",
            "Epoch: 625/1000... Step: 36864... Loss: 0.816919...\n",
            "Epoch: 630/1000... Step: 37120... Loss: 0.857243...\n",
            "Epoch: 634/1000... Step: 37376... Loss: 0.897701...\n",
            "Epoch: 638/1000... Step: 37632... Loss: 0.921476...\n",
            "Epoch: 643/1000... Step: 37888... Loss: 0.873075...\n",
            "Epoch: 647/1000... Step: 38144... Loss: 0.834112...\n",
            "Epoch: 651/1000... Step: 38400... Loss: 0.802446...\n",
            "Epoch: 656/1000... Step: 38656... Loss: 0.727885...\n",
            "Epoch: 660/1000... Step: 38912... Loss: 0.894825...\n",
            "Epoch: 664/1000... Step: 39168... Loss: 0.760504...\n",
            "Epoch: 669/1000... Step: 39424... Loss: 0.831589...\n",
            "Epoch: 673/1000... Step: 39680... Loss: 0.888645...\n",
            "Epoch: 677/1000... Step: 39936... Loss: 0.839443...\n",
            "Epoch: 682/1000... Step: 40192... Loss: 0.837241...\n",
            "Epoch: 686/1000... Step: 40448... Loss: 0.928738...\n",
            "Epoch: 690/1000... Step: 40704... Loss: 0.877509...\n",
            "Epoch: 695/1000... Step: 40960... Loss: 0.680521...\n",
            "Epoch: 699/1000... Step: 41216... Loss: 0.826779...\n",
            "Epoch: 703/1000... Step: 41472... Loss: 0.809430...\n",
            "Epoch: 708/1000... Step: 41728... Loss: 0.937110...\n",
            "Epoch: 712/1000... Step: 41984... Loss: 0.752222...\n",
            "Epoch: 716/1000... Step: 42240... Loss: 0.930951...\n",
            "Epoch: 721/1000... Step: 42496... Loss: 0.822744...\n",
            "Epoch: 725/1000... Step: 42752... Loss: 0.811347...\n",
            "Epoch: 729/1000... Step: 43008... Loss: 0.993184...\n",
            "Epoch: 734/1000... Step: 43264... Loss: 0.901187...\n",
            "Epoch: 738/1000... Step: 43520... Loss: 0.803075...\n",
            "Epoch: 742/1000... Step: 43776... Loss: 0.762636...\n",
            "Epoch: 747/1000... Step: 44032... Loss: 0.789933...\n",
            "Epoch: 751/1000... Step: 44288... Loss: 0.875771...\n",
            "Epoch: 755/1000... Step: 44544... Loss: 0.820987...\n",
            "Epoch: 760/1000... Step: 44800... Loss: 0.908958...\n",
            "Epoch: 764/1000... Step: 45056... Loss: 0.682154...\n",
            "Epoch: 768/1000... Step: 45312... Loss: 0.788794...\n",
            "Epoch: 773/1000... Step: 45568... Loss: 0.943351...\n",
            "Epoch: 777/1000... Step: 45824... Loss: 0.974784...\n",
            "Epoch: 782/1000... Step: 46080... Loss: 0.634903...\n",
            "Epoch: 786/1000... Step: 46336... Loss: 0.806605...\n",
            "Epoch: 790/1000... Step: 46592... Loss: 0.818168...\n",
            "Epoch: 795/1000... Step: 46848... Loss: 0.938363...\n",
            "Epoch: 799/1000... Step: 47104... Loss: 0.775318...\n",
            "Epoch: 803/1000... Step: 47360... Loss: 0.823981...\n",
            "Epoch: 808/1000... Step: 47616... Loss: 0.971679...\n",
            "Epoch: 812/1000... Step: 47872... Loss: 0.828881...\n",
            "Epoch: 816/1000... Step: 48128... Loss: 0.930534...\n",
            "Epoch: 821/1000... Step: 48384... Loss: 0.910094...\n",
            "Epoch: 825/1000... Step: 48640... Loss: 0.889057...\n",
            "Epoch: 829/1000... Step: 48896... Loss: 0.856401...\n",
            "Epoch: 834/1000... Step: 49152... Loss: 0.753492...\n",
            "Epoch: 838/1000... Step: 49408... Loss: 0.846583...\n",
            "Epoch: 842/1000... Step: 49664... Loss: 0.851327...\n",
            "Epoch: 847/1000... Step: 49920... Loss: 0.892045...\n",
            "Epoch: 851/1000... Step: 50176... Loss: 0.843467...\n",
            "Epoch: 855/1000... Step: 50432... Loss: 0.867893...\n",
            "Epoch: 860/1000... Step: 50688... Loss: 0.771597...\n",
            "Epoch: 864/1000... Step: 50944... Loss: 0.851787...\n",
            "Epoch: 868/1000... Step: 51200... Loss: 0.789596...\n",
            "Epoch: 873/1000... Step: 51456... Loss: 0.819394...\n",
            "Epoch: 877/1000... Step: 51712... Loss: 0.933711...\n",
            "Epoch: 881/1000... Step: 51968... Loss: 0.889905...\n",
            "Epoch: 886/1000... Step: 52224... Loss: 0.576926...\n",
            "Epoch: 890/1000... Step: 52480... Loss: 0.893899...\n",
            "Epoch: 894/1000... Step: 52736... Loss: 0.829521...\n",
            "Epoch: 899/1000... Step: 52992... Loss: 0.756360...\n",
            "Epoch: 903/1000... Step: 53248... Loss: 0.778031...\n",
            "Epoch: 907/1000... Step: 53504... Loss: 0.928812...\n",
            "Epoch: 912/1000... Step: 53760... Loss: 0.739598...\n",
            "Epoch: 916/1000... Step: 54016... Loss: 0.835000...\n",
            "Epoch: 920/1000... Step: 54272... Loss: 0.806228...\n",
            "Epoch: 925/1000... Step: 54528... Loss: 0.860653...\n",
            "Epoch: 929/1000... Step: 54784... Loss: 0.949317...\n",
            "Epoch: 933/1000... Step: 55040... Loss: 0.868711...\n",
            "Epoch: 938/1000... Step: 55296... Loss: 0.774277...\n",
            "Epoch: 942/1000... Step: 55552... Loss: 0.900288...\n",
            "Epoch: 946/1000... Step: 55808... Loss: 0.979052...\n",
            "Epoch: 951/1000... Step: 56064... Loss: 0.840605...\n",
            "Epoch: 955/1000... Step: 56320... Loss: 0.814820...\n",
            "Epoch: 959/1000... Step: 56576... Loss: 0.943769...\n",
            "Epoch: 964/1000... Step: 56832... Loss: 0.871382...\n",
            "Epoch: 968/1000... Step: 57088... Loss: 0.772929...\n",
            "Epoch: 972/1000... Step: 57344... Loss: 0.950852...\n",
            "Epoch: 977/1000... Step: 57600... Loss: 0.821501...\n",
            "Epoch: 981/1000... Step: 57856... Loss: 0.885140...\n",
            "Epoch: 985/1000... Step: 58112... Loss: 0.915580...\n",
            "Epoch: 990/1000... Step: 58368... Loss: 0.859923...\n",
            "Epoch: 994/1000... Step: 58624... Loss: 0.872505...\n",
            "Epoch: 998/1000... Step: 58880... Loss: 0.835031...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1997iM_psXaP"
      },
      "source": [
        "# save the model\n",
        "torch.save(model.state_dict(), \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/LSTM_0.pt\")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASr80p94sXPA",
        "outputId": "928729b5-ff84-4176-a02f-5d73aa18c1b0"
      },
      "source": [
        "test_losses = []\n",
        "num_correct = 0\n",
        "h = model.init_hidden(batch_size)\n",
        "\n",
        "model.eval()\n",
        "for inputs, labels in test_loader:\n",
        "    \n",
        "    h = tuple([each.data for each in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    output, h = model(inputs, h)\n",
        "    fetched_labels = labels.data.tolist()\n",
        "    fetched_output = output.squeeze().data.tolist()\n",
        "    #print(len(fetched_labels)) # 32\n",
        "    #print(len(fetched_output[0])) # 5\n",
        "\n",
        "    # absolute mean average\n",
        "    denominator = len(output)\n",
        "    for i in range(denominator):\n",
        "      diff = abs( fetched_labels[i] - ( fetched_output[i].index( max(fetched_output[i]) ) + 1 ) )\n",
        "      if diff == 0:\n",
        "        num_correct += 1\n",
        "      test_losses.append(diff)\n",
        "\n",
        "print(test_losses)        \n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 2, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 2, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 2, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 2, 1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 2, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 0, 1, 1, 1, 2, 0, 1, 1, 1, 0, 1, 1, 0, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 0, 0, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 2, 1, 1, 0, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 2, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1]\n",
            "Test loss: 0.867\n",
            "Test accuracy: 18.430%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w52OdjZtBS_"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zi2FuT5tBQn"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    }
  ]
}